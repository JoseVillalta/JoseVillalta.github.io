<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Yes Way Jose</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Yes Way Jose</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Mar 2025 08:49:27 -0800</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Sirens Call: How Attention Became the World&#39;s Most Endangered Resource</title>
      <link>http://localhost:1313/posts/the-sirens-call/</link>
      <pubDate>Wed, 05 Mar 2025 08:49:27 -0800</pubDate>
      
      <guid>http://localhost:1313/posts/the-sirens-call/</guid>
      <description>&lt;h3 id=&#34;book-review&#34;&gt;Book Review&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;m almost finished with &lt;a href=&#34;https://www.amazon.com/Sirens-Call-Attention-Endangered-Resource-ebook/dp/B0DDNGLSJP&#34;&gt;The Siren&amp;rsquo;s Call by Chris Hayes&lt;/a&gt;
and it&amp;rsquo;s interesting enough to call out here.&lt;/p&gt;
&lt;p&gt;The book starts with an excellent explanation of what attention is and how it works. Then the book presents two analogies to use when thinking about attention.&lt;/p&gt;
&lt;p&gt;The first one is to think of Attention as a resource that drives the economy, like labor, it is commodified, it can be monetized, marketed bought and sold.
Since information is now pletiful, (there&amp;rsquo;s too much of it, really) attention is scarce, we have a limit of how much information we consume, how much we can pay attention to.
So now market forces, tech companies, politicians they are all competing for our attention. Our attention has value, it yields money, power and fame.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How do you architech change? The answer is simple, but not always easy</title>
      <link>http://localhost:1313/posts/useful-strategies-for-growing/</link>
      <pubDate>Sat, 01 Mar 2025 16:13:55 -0800</pubDate>
      
      <guid>http://localhost:1313/posts/useful-strategies-for-growing/</guid>
      <description>&lt;p&gt;This week I had an Eureka moment at work. You see, there are many things I see at work that I would like to improve. Technical debt in our codebase. Inefficient processes. Communication silos across teams. The list of things one can improve never ends, this is true in all software shops. System complixity grows as new functionality gets added, inefficiencies optimized, bugs fixed. Secuirty hardened, etc.&lt;/p&gt;
&lt;p&gt;How do you increase the quality of your system without interrupting the flow? Well, obviously, you break it up, one tiny thing at time. That&amp;rsquo;s how. Yeah it&amp;rsquo;s obvious but (this is embarrasing to admit) I get the urge to make BIG changes, I&amp;rsquo;d like to rewrite whole chuncks of the codebase, I&amp;rsquo;d like to build a brand new release pipeline, and we might do that someday, but not today. When you have a team that&amp;rsquo;s busy doing the work, tidying up gets deprioritized. After all, that bug in prod needs to get fixed yesterday, that new feature needs to ship on time. Oh, by the way, the developer that was working on that thing your system depends on quit last week.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Learning Go</title>
      <link>http://localhost:1313/posts/learning-go/</link>
      <pubDate>Tue, 27 Aug 2024 08:32:11 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/learning-go/</guid>
      <description>&lt;p&gt;I have been an official Go programmer for three years now. Unlike many people in my team, I remember the day Google announced go. I don&amp;rsquo;t remember if it was in Hacker News or /programming reddit, but I do remember watching the go math package compiling in less than a second, at the time, I was writing C++ for an embedded system. Building the whole model as we used to say took 45 minutes, this compiled our C/C++ project into a .out file for an ARM9 and a C55 DSP. When I saw how quickly Go built I was like, wow. To be fair, our build was for a Real Time OS so it didn&amp;rsquo;t even include the C++ standard library. Most of the time, if I remember right, was spent linking everything. THe linker was getting it&amp;rsquo;s poor butt kicked. Anyyway, I looked at Rob Pike (?) on YouTube and I was like,&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Awesome Falsehoods Programmers Believe</title>
      <link>http://localhost:1313/posts/awesome-falsehoods/</link>
      <pubDate>Wed, 08 Nov 2023 07:57:58 -0800</pubDate>
      
      <guid>http://localhost:1313/posts/awesome-falsehoods/</guid>
      <description>&lt;h3 id=&#34;a-curated-list-of-falsehoods-programmers-believe-in&#34;&gt;A curated list of falsehoods programmers believe in.&lt;/h3&gt;
&lt;p&gt;The Code we write is a representation of the things we believe are true about the world. Every one has just one name, right? Well, most of the time, yes. All of the time? No. If you write an app for yourself, or your small business most of these assumptions are fine. If you write code for millions of users, you are going to find exceptions. Some of them surprise me. Dealing with Time sucks, dealing with addresses, names, supporting multiple languages it&amp;rsquo;s really hard to get it right 100% of the time.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Book Summary: Thinking in Systems by Donella Meadows</title>
      <link>http://localhost:1313/posts/systems-book-notes/</link>
      <pubDate>Sat, 04 Nov 2023 16:18:25 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/systems-book-notes/</guid>
      <description>&lt;h2 id=&#34;book-summary-thinking-in-systems&#34;&gt;Book Summary: Thinking in Systems&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s say you want to build the perfect self-driving vehicle. This thing you want to make is composed of many parts. There&amp;rsquo;s the car itself made up of many subparts (engine, tires, transmission, etc) as well as the AI tech. Realistically you will need a bunch of speciallized embedded systems as well as a central computer to orchestrate everything. There&amp;rsquo;s sensors, actuators, orchestrators, etc.
In short, how does one person know if a design is good? How do you know how fast the visual recognition needs to be to handle controlling a car going 60 mph?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The top 3 podcasts for Software Developers</title>
      <link>http://localhost:1313/posts/best-podcasts/</link>
      <pubDate>Tue, 31 Oct 2023 09:05:57 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/best-podcasts/</guid>
      <description>&lt;h3 id=&#34;the-top-3-podcasts-for-software-developers&#34;&gt;The Top 3 Podcasts for Software Developers&lt;/h3&gt;
&lt;h4 id=&#34;go-time-by-changelog&#34;&gt;Go Time by Changelog&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://changelog.com/gotime&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is the podcast to keep up-to-date with all things Go. The jokes are nerdy and the hosts are sometimes not as funny as they think they are, but the content is great and they have a wide set of guests in the show that make it a must for all people who write go for a living&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Article Review: Lessons Learned from Twenty Years of Site Reliability Engineering</title>
      <link>http://localhost:1313/posts/twenty-years-of-sre-lessons-learned/</link>
      <pubDate>Tue, 31 Oct 2023 08:29:26 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/twenty-years-of-sre-lessons-learned/</guid>
      <description>&lt;h3 id=&#34;article-lessons-learned-from-twenty-years-of-site-reliability-engineering&#34;&gt;Article: Lessons Learned from Twenty Years of Site Reliability Engineering&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://sre.google/resources/practices-and-processes/twenty-years-of-sre-lessons-learned/&#34;&gt;Link to article&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The site realibility team at Google put together a summary of the lessons they have learned over the years. I am glad they decided to share. The best way to learn is by trial and error. Want your product or service to be better? Launch it, monitor it, and learn from the mistakes. It nice to learn from others, but there is no substitue to first hand experience.  This is the list they came up with:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Staff Engineer Path</title>
      <link>http://localhost:1313/posts/staff-engineer-path/</link>
      <pubDate>Mon, 24 Jul 2023 09:05:00 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/staff-engineer-path/</guid>
      <description>&lt;h3 id=&#34;book-review-the-staff-engineers-path-by-tanya-reilly&#34;&gt;Book Review: The Staff Engineer&amp;rsquo;s Path by Tanya Reilly&lt;/h3&gt;
&lt;p&gt;Tanya Reilly gives a guide for individual contributor software engineers who wish to grow their career but do not want to become managers. It gives insights about what a staff engineer does, and what you need to do to perform at that level. This is a technology-agnostic book. It gives the reader a high level view of the functional areas that matter.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Coroutines for Go</title>
      <link>http://localhost:1313/posts/coroutines-for-go/</link>
      <pubDate>Tue, 18 Jul 2023 06:51:45 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/coroutines-for-go/</guid>
      <description>&lt;h3 id=&#34;go&#34;&gt;Go&lt;/h3&gt;
&lt;p&gt;Russ Cox put out an &lt;a href=&#34;https://research.swtch.com/coro&#34;&gt;article&lt;/a&gt; yesteday about adding the abilities to run coroutines in go. Today I learned the difference
between a &lt;a href=&#34;https://go.dev/tour/concurrency/1&#34;&gt;goroutine&lt;/a&gt; and a &lt;a href=&#34;https://en.wikipedia.org/wiki/Coroutine&#34;&gt;coroutine&lt;/a&gt;. Coroutine is a concurrency pattern in which only one runs at a time. Say we have coroutine A and B. B waits while A runs then A yields to B and A waits while B runs. It turns out this is useful in a few scenarios.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Learning File Systems</title>
      <link>http://localhost:1313/posts/learning-file-systems/</link>
      <pubDate>Sat, 15 Jul 2023 21:21:31 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/learning-file-systems/</guid>
      <description>&lt;p&gt;Lately I have been learning about File Systems from the book &amp;ldquo;Operating Systems. Three Easy Pieces&amp;rdquo; by Remzi Arpaci-Dusseau.&lt;/p&gt;
&lt;p&gt;I used to think that I knew how file systems worked because the interface open, read and write is so straight forward, what else could there be to it? But then at work some weird issues come up where some weird behaviour happens, like, du says the disk has space but df says the disk is full, what could make that happen? Or when an issue mounting a volume occurs and you realize that you don&amp;rsquo;t know the difference between mounting a block device versus mounting a file system. Are they both the same thing? I need to have a mental model of what the system is doing in order to debug it. Knowing the data structures in the file system and having an idea of what happens when you open a file, how does the operating system find the file? how does it traverse the file tree? What is in memory versus disk?
I must confess I am still in the dark when it comes to container images and union file systems. I understand how a container is made up of many layers over imposed on top of each other. An image is essentially a tar file of different file systems on top of each other. But, how is it implemented? how do you, can you just put a whole other /proc and other system files on top of a kernel?
So anyway, that&amp;rsquo;s what I have been up to.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>My Tsundoku Pile</title>
      <link>http://localhost:1313/posts/my-tsundoku-pile/</link>
      <pubDate>Sun, 22 Jan 2023 20:02:32 -0800</pubDate>
      
      <guid>http://localhost:1313/posts/my-tsundoku-pile/</guid>
      <description>&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/pile.jpg&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;I&amp;rsquo;m trying to get through all my technical books that I&amp;rsquo;ve adquired, and never gotten around to.
Not going to lie, the Knuth books are intimidating. They are actually not that bad to get through, but they are books that I pick up, read a few pages on a specfic project, try to do a problem or two, and that&amp;rsquo;s it.&lt;/p&gt;
&lt;p&gt;The other books are less intimidating, more doable, I&amp;rsquo;m pretty sure al but one of these books were lying around the Amazon campus just sitting on shelves.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper every Day. Day ten: The Unix Timesharing System by Dennis Ritchie and Ken Thompson</title>
      <link>http://localhost:1313/posts/day-ten-unix-timesharing/</link>
      <pubDate>Sun, 31 Jul 2022 10:40:30 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/day-ten-unix-timesharing/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://dsf.berkeley.edu/cs262/unix.pdf&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This paper, published in July 1974 is remarkable because the design decisions that were made back then by these guys working at Bell Labs on an operating system for the &lt;a href=&#34;https://en.wikipedia.org/wiki/PDP-11&#34;&gt;PDP-11 &lt;/a&gt; are still relevant.&lt;/p&gt;
&lt;p&gt;I am still struggling to create a mental model of the unix file system, the fact that it looks like a single tree with the root at the top while simultaneously you can have multiple devices &lt;a href=&#34;https://en.wikipedia.org/wiki/Mount_(Unix)&#34;&gt;mounted&lt;/a&gt; dates back to these guys at Bell Labs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper every day: Day Nine: An Analysis of Linux Scalability to Many Cores</title>
      <link>http://localhost:1313/posts/day-nine-linux-scalability/</link>
      <pubDate>Mon, 25 Jul 2022 09:13:41 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/day-nine-linux-scalability/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pdos.csail.mit.edu/papers/linux:osdi10.pdf&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;From the abstract:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This paper analyzes the scalability of seven system applications running on Linux on a 48-core computer&amp;hellip;using mostly standard parallel programming techniques -this paper introduces one new technique &lt;strong&gt;sloppy counters&lt;/strong&gt; these bottlencek can be removed from the kernl or avoided by changing the application slightly&amp;rdquo;&lt;/p&gt;
&lt;p&gt;This paper has an excellent system level tutorial on scalability. They explain that you don&amp;rsquo;t get linear increase in performance because in real life applications parallel tasks usually interact, an interaction forces serial execution. Then they list the common causes with common solutions. This paper is throughly written and researched. Writing truly parallel code is difficult and even then applications still compete for some shared resouce, be it a local cache, network access or disk I/O.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lets Go</title>
      <link>http://localhost:1313/posts/lets-go/</link>
      <pubDate>Sun, 24 Jul 2022 21:42:07 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/lets-go/</guid>
      <description>&lt;h3 id=&#34;go&#34;&gt;Go&lt;/h3&gt;
&lt;p&gt;I have been writing go since a little after last year. I actually remember hearing about go when it first came out, back then I honestly never thought I&amp;rsquo;d be getting paid to work in it.&lt;/p&gt;
&lt;p&gt;Even though I&amp;rsquo;ve been writing code in go for a while, I don&amp;rsquo;t think I know the language in enough depth to consider myself a go expert. I want to change that. So I am going to start writing about go here as a way to &amp;ldquo;learn in public&amp;rdquo;
Expect posts on the following topics:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper every day. Day Eight. Omega: flexible scalable scheduler for large compute clusters</title>
      <link>http://localhost:1313/posts/day-eight-omega/</link>
      <pubDate>Sun, 24 Jul 2022 21:30:27 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/day-eight-omega/</guid>
      <description>&lt;h3 id=&#34;omega&#34;&gt;Omega&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41684.pdf&#34;&gt;Link to paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Omega was the second cluster manager system built by Google. It is Borg&amp;rsquo;s sucessor and it was designed as a happy medium between Borg&amp;rsquo;s centralized scheduler architecture and Mesos&amp;rsquo;s two-level approach where the placement is delegated to the running framework. Omega shares the state of the cluster among leaders and uses optimistic concurrency control (detect when different cluster schedulers are competing for the same resource)&lt;/p&gt;
&lt;p&gt;The premise of the whole paper is that a centralized scheduler does not scale well, so there must be a better way to handle scheduling different types of workloads in a fast and conrrect manner. The two main types of workloads, services and batches have different requriements and present their unique challenges. The paper explains the type of simulations the engineer at Google used to determine that conflicts among different scheduler is not that common and that Omega manages to fit more tasks in the clusters than Mesos.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper every day. Day Seven: Large-scale cluster management at Google with Borg</title>
      <link>http://localhost:1313/posts/day-seven-borg/</link>
      <pubDate>Sat, 23 Jul 2022 18:56:09 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/day-seven-borg/</guid>
      <description>&lt;h3 id=&#34;borg&#34;&gt;Borg&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43438.pdf&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Borg is the cluster management system that runs hundreds of thousands of jobs at Google, it is the original system, it&amp;rsquo;s sucessor Omega was written as a reaction to the lessons learned from it. Kubernetes is the third system written with the lessons from those two. This paper helped me understand a few things about my own system since we have our own cluster managenet and scheduler system that work a little different but in general do the same job.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper every day. Day Six: Hints for Computer Design</title>
      <link>http://localhost:1313/posts/day-six-hints-computer-design/</link>
      <pubDate>Fri, 22 Jul 2022 08:13:55 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/day-six-hints-computer-design/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/acrobat-17.pdf&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This paper was published originally in 1983 by the legendary folks from the Xerox Palo Alto Research Center.
The hints and tips should sound familiar but it&amp;rsquo;s interesting to notice the &lt;strong&gt;layer&lt;/strong&gt; the author is talking about, these guys were designing at very low level.
The fact that the same rules apply now it&amp;rsquo;s remarkable. It turns out breaking up a system into the right abstraction with a good interface it&amp;rsquo;s rather hard.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper every day. Day Five: On Designing and Deploying Internet Scale Services</title>
      <link>http://localhost:1313/posts/day-five-design-internet-scale/</link>
      <pubDate>Thu, 21 Jul 2022 07:07:19 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/day-five-design-internet-scale/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://s3.amazonaws.com/systemsandpapers/papers/hamilton.pdf&#34;&gt;Link to Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This paper is a list of recommendations for running a large-scale system that aims to keep quality high and costs low. The author comes from the Windows Live Services Platform but this might as well be read as an Amazon internal guide since I didn&amp;rsquo;t see a thing in this list that we don&amp;rsquo;t do (or aim to do) in AWS. EDIT: Of course all these things sound familiar! The author is &lt;a href=&#34;https://perspectives.mvdirona.com/&#34;&gt;James Hamilton!&lt;/a&gt; He&amp;rsquo;s a VP and distinguished engineer here at AWS, I know him from leading the weekly AWS Ops Review meeting.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper every day. Day Four: Harvest, Yield, and Scalable Tolerant Systems</title>
      <link>http://localhost:1313/posts/day-four-harvest-yield/</link>
      <pubDate>Wed, 20 Jul 2022 06:28:24 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/day-four-harvest-yield/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://s3.amazonaws.com/systemsandpapers/papers/FOX_Brewer_99-Harvest_Yield_and_Scalable_Tolerant_Systems.pdf&#34;&gt;Link to paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Today&amp;rsquo;s paper comes thanks to &lt;a href=&#34;https://lethain.com/&#34;&gt;Will Larson&lt;/a&gt; this is a recommended paper in his book &lt;a href=&#34;https://www.amazon.com/dp/B07QYCHJ7V/&#34;&gt;Elegant Puzzle&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This paper builds on the concepts from the the &lt;a href=&#34;https://en.wikipedia.org/wiki/CAP_theorem&#34;&gt;CAP Theorem&lt;/a&gt; which essentially says that when it comes to distributed systems you can only have 2 out of these 3 qualities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Consistency&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Avalaibility&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Partition Tolerance&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then it introduces two concepts &lt;strong&gt;harvest&lt;/strong&gt; and &lt;strong&gt;yield&lt;/strong&gt; which is interesting because it&amp;rsquo;s not something you usually hear in distributed systems (this is a paper from the &amp;rsquo;90s) and yet I think it&amp;rsquo;s import to know and to think in these terms.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper every day. Day Three: Container Design Patterns</title>
      <link>http://localhost:1313/posts/day-three-container-design-patterns/</link>
      <pubDate>Tue, 19 Jul 2022 07:23:34 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/day-three-container-design-patterns/</guid>
      <description>&lt;h3 id=&#34;day-three-design-patterns-for-container-based-distributed-systems&#34;&gt;Day three: Design patterns for container-based distributed systems&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45406.pdf&#34;&gt;Link to paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Containers provide the ability to package, deploy and reuse applications using a natural isolation boundary. Developers can expose application-specific functionality as interfaces as well as more generic hooks for many systems like metrics, health, etc.&lt;/p&gt;
&lt;p&gt;In this paper the authors present the readers with two types of container design patterns: Patterns for container in the same machine (Single node) and patterns where the containers are spread out in different machines (multi node)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Paper every day. Day Two: Kubernetes</title>
      <link>http://localhost:1313/posts/day-two-k8s/</link>
      <pubDate>Mon, 18 Jul 2022 06:33:49 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/day-two-k8s/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Welcome to day 2 of my paper-every-day journey. Today we&amp;rsquo;re going to cover &lt;a href=&#34;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44843.pdf&#34;&gt;Borg, Omega and Kubernetes&lt;/a&gt; This paper goes over how Kubernetes, the de-facto, open-source orchestrator of containers was developed using the lessons learned from building Borg and Omega, Google&amp;rsquo;s internal job orchestrator.&lt;/p&gt;
&lt;p&gt;Kubernetes is a container orchestration system that aims to make developing and deploying complex distributed systems easier.
The Borg and Omega papers are excellent papers on their own right, but this explanation original published in ACM Queue is very insightful. These are the lessons I gathered:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Goal: A Paper Every Day. Day One: Mesos Paper</title>
      <link>http://localhost:1313/posts/day-one-paper/</link>
      <pubDate>Sun, 17 Jul 2022 13:04:01 -0700</pubDate>
      
      <guid>http://localhost:1313/posts/day-one-paper/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve decided to read a research paper each day. I am doing this in order to get better at my craft.
I want to be better at designing software and it has been shown in research that the people who are really good at what they do
are those that do &amp;ldquo;delibarate practice&amp;rdquo; I am most definitely NOT the best software system designer out there, but talent is overated and I
intend to improve my knowledge one day at time. You are welcome to follow along.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/about/</guid>
      <description>&lt;p&gt;I am a Software Engineer at Amazon, working on the &lt;a href=&#34;https://aws.amazon.com/blogs/containers/under-the-hood-fargate-data-plane/&#34;&gt;AWS ECS Fargate Dataplane.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Prior to Amazon, I worked at &lt;a href=&#34;https://www.impinj.com/&#34;&gt;Impinj&lt;/a&gt;, a startup (at the time) that makes RFID chips. I worked on the software that tests the chips at the factory. This is where I made the jump from writing embedded software (C++, Wireless signal concepts) to Web Services (Ruby on Rail Applications backed by a MS-SQL Database) I learned about Containers here. I led the effort to break up a monolith back-end application into smaller, manageble microservices. We used Docker.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>New blog!</title>
      <link>http://localhost:1313/posts/sample/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/sample/</guid>
      <description>&lt;p&gt;Started a new page to write about tech stuff. Work in Progress&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Professional Experience and Education</title>
      <link>http://localhost:1313/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/experience/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;h4 id=&#34;amazon-2018---present&#34;&gt;Amazon 2018 - present&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;AWS Fargate: Dataplane team. Writing Golang mostly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Worked on retail site for 2.5 years using the away team model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;impinj-2014---2018&#34;&gt;Impinj 2014 - 2018&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Wrote Ruby on Rails&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;motorola-2007---2014&#34;&gt;Motorola 2007 - 2014&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Low level firmware&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;co-op-intern-at-ibm-2005---2006&#34;&gt;Co-op Intern at IBM 2005 - 2006&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ASIC design model testing&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;education&#34;&gt;Education&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Master&amp;rsquo;s Degree in Electrical and Computer Engineer from the University of Florida&lt;/li&gt;
&lt;li&gt;Bachelor&amp;rsquo;s Degree in Computer Engineering from Florida Atlantic University&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;my-career-journey&#34;&gt;My Career Journey&lt;/h2&gt;
&lt;p&gt;This is a more detailed narrative of my carreer so far. I&amp;rsquo;ve been lucky to work on cool projects solving interesting problems.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
