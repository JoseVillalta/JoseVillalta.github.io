<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Deep Dive into Network Namespaces in AWS ECS Containers | Yes Way Jose</title><meta name=keywords content="aws,ecs,containers,networking,linux,namespaces"><meta name=description content="Ever wondered what happens under the hood when you launch an ECS task with awsvpc networking? Let's explore the fascinating world of network isolation in AWS ECS."><meta name=author content="Jose Villalta"><link rel=canonical href=https://JoseVillalta.github.io/posts/network-namespaces-ecs-containers/><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.555af97124d54bb1457985dd081b8f5616a48103aafeb30ac89fde835d65aa6c.js integrity="sha256-VVr5cSTVS7FFeYXdCBuPVhakgQOq/rMKyJ/eg11lqmw=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://JoseVillalta.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JoseVillalta.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JoseVillalta.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JoseVillalta.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JoseVillalta.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://JoseVillalta.github.io/posts/network-namespaces-ecs-containers/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="A Deep Dive into Network Namespaces in AWS ECS Containers"><meta property="og:description" content="Ever wondered what happens under the hood when you launch an ECS task with awsvpc networking? Let's explore the fascinating world of network isolation in AWS ECS."><meta property="og:type" content="article"><meta property="og:url" content="https://JoseVillalta.github.io/posts/network-namespaces-ecs-containers/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-12T17:12:00-07:00"><meta property="article:modified_time" content="2025-10-12T17:12:00-07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Deep Dive into Network Namespaces in AWS ECS Containers"><meta name=twitter:description content="Ever wondered what happens under the hood when you launch an ECS task with awsvpc networking? Let's explore the fascinating world of network isolation in AWS ECS."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://JoseVillalta.github.io/posts/"},{"@type":"ListItem","position":2,"name":"A Deep Dive into Network Namespaces in AWS ECS Containers","item":"https://JoseVillalta.github.io/posts/network-namespaces-ecs-containers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Deep Dive into Network Namespaces in AWS ECS Containers","name":"A Deep Dive into Network Namespaces in AWS ECS Containers","description":"Ever wondered what happens under the hood when you launch an ECS task with awsvpc networking? Let's explore the fascinating world of network isolation in AWS ECS.","keywords":["aws","ecs","containers","networking","linux","namespaces"],"articleBody":"How do multiple containers run on the same EC2 instance without stepping on each other’s network traffic? The answer lies in Linux network namespaces—a powerful isolation mechanism that gives each ECS task its own private network stack. Join me as we dissect a live ECS instance and uncover the elegant engineering that makes awsvpc networking possible.\nWhat are network namespaces for? A Linux namespace is a construct that creates an isolated copy of the networking stack, enabling them to run on the same host with different IP addresses, DNS configurations, and route tables. This allows multiple containers to run on the same machine without interfering with each other’s network traffic.\nIn ECS, when you want to run containerized applications, you create a task that can contain up to 10 containers. All containers within a task share the same network namespace. Each managed instance supports multiple network interfaces (ENIs) attached, task ENIS are provisioned by ECS Control plane at task launch time.\nLet’s take a look under the hood.\nSetting Up the Investigation I will launch an EC2 instance using an AMI provisioned with the ECS Managed Instance Agent running on Bottlerocket. This agent runs the same dataplane software that powers production instances. To enable debugging access, I’ll create a variant that includes the login and SSM packages, allowing me to connect via the EC2 Serial Console. The instance will launch in EC2 debug mode, which means it won’t be managed by the ECS Control Plane but will be fully owned by my account.\nThe AWS Console shows that I have an EC2 instance with two ENIs attached. I have two private IPs associated with each ENI. 10.194.20.168 is my task IP and 10.184.20.158 is the IP address of my host. The primary ENI is listed as Index 0 and the task ENI is listed as Index 1.\nExamining Network Interfaces from the Host Connecting to the instance as the root user and running ip link show produces the following output:\nip link show 1: lo: mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 02:df:6a:de:12:21 brd ff:ff:ff:ff:ff:ff altname enp0s5 altname ens5 4: fargate-bridge: mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 2a:56:f8:7a:ea:8e brd ff:ff:ff:ff:ff:ff 5: veth752433c6@if3: mtu 1500 qdisc noqueue master fargate-bridge state UP mode DEFAULT group default link/ether a2:03:66:23:29:82 brd ff:ff:ff:ff:ff:ff link-netns a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 The ip link show command gives you the list of network interfaces in a Linux system. Notice that it only shows one ENI “eth0” but you don’t see the task ENI “eth1”.\nLet’s confirm that eth0 is my actual primary ENI by comparing its address with the info from my AWS Console:\nbash-5.2# ip address show 1: ... 2: eth0: mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 02:df:6a:de:12:21 brd ff:ff:ff:ff:ff:ff altname enp0s5 altname ens5 inet 10.194.20.158/24 metric 1024 brd 10.194.20.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::df:6aff:fede:1221/64 scope link proto kernel_ll valid_lft forever preferred_lft forever ... Yep, I see that the IP for eth0 is 10.194.20.158/24 and I see the MAC address matches. I also noticed that eth1 is not visible from the primary network namespace—as far as the host is concerned there is only one gateway to the internet.\nExploring Network Namespaces Let’s get a list of the network namespaces running:\nbash-5.2# ip netns show a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 (id: 0) bash-5.2# ctr -n fargate.task t ls TASK PID STATUS a333f40b6ac74e92b1541fb0a5d76f9e-0883211837 1551 RUNNING bash-5.2# The first command ip netns show gives me a list of all the network namespaces. The second command is ctr, a command line client to talk to the containerd daemon. t ls lists all the tasks running in the fargate.task namespace.\nYes, we’re running the Fargate Agent under the hood :)\nLet’s see what the namespace looks like from inside the network namespace (netns) of my awsvpc task. I’ll use a nifty tool called ip netns exec, which allows me to launch commands inside a network namespace.\n# ip netns exec a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 ip link show 1: lo: mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if5: mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 0a:58:a9:fe:ac:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 4: eth1: mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 02:ed:2d:97:4f:29 brd ff:ff:ff:ff:ff:ff altname enp0s6 altname ens6 Well, there it is. eth1 is here! Notice that eth0 is not here. As far as the container namespace is concerned, there’s only one ENI in this host.\nLet’s confirm by looking at the IP address assigned to eth1:\n# ip netns exec a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 ip address show 1:... 3:... 4: eth1: mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 02:ed:2d:97:4f:29 brd ff:ff:ff:ff:ff:ff altname enp0s6 altname ens6 inet 10.194.20.168/24 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::ed:2dff:fe97:4f29/64 scope link proto kernel_ll valid_lft forever preferred_lft forever Yes, eth1 has 10.194.20.168 which matches what the AWS console tells me.\nThe Complete Network Picture Okay, but what about the other stuff I see in the IP command output?\nYes, good question. This is not obvious, but there’s one more thing: there’s a bridge from the task namespace that connects it to the task metadata service (TMDS). The namespace is connected via a veth pair. The full namespace picture looks like this:\nNow we see that eth0@if5 is a veth (virtual ethernet) device connected to another veth in the primary interface that will route MAC frames to the fargate-bridge which acts as a virtual switch and will route MAC traffic to the task metadata server (TMDS) running on my primary namespace.\nWhat else goes into a netns? Besides the network interface we need to configure the namespace with everything it needs to work as an effectual copy of a Linux network stack, with its own routes, firewall rules and devices. In awsvpc tasks this means you get:\nA loopback interface DNS configuration files Routes in the iptables A veth interface that connects to the primary netns How to configure network namespaces using IP commands One way to configure network namespaces is to use the IP command to create and configure the network namespace.\nTo configure a namespace like awsvpc, you need to create the netns and configure the ENI, you need to configure DNS and create a bridge from the netns to the task metadata server.\nTo create a netns use ip netns add command:\nsudo ip netns add my_container_ns This command creates a new, empty network namespace named my_container_ns. It will appear as a mount point under /var/run/netns. [1]\nIf you want to configure an ENI you’d use ip link commands to create, setup, add IP address, set MTU, etc. For illustrative purposes I’ve added some scripts created with generative AI.\nThis looks about right (use at your own risk, this is untested code).\nTo create a namespace with a bridge run something like this:\n#!/bin/bash # This script creates a network namespace, a veth pair, and a bridge between # the host and the network namespace. set -e # --- Configuration --- NS_NAME=\"testns\" # Name of the network namespace VETH_HOST=\"veth-host\" # veth interface in the root namespace VETH_NS=\"veth-ns\" # veth interface inside the namespace BR_NAME=\"br0\" # Name of the bridge IP_HOST=\"10.200.1.1/24\" # IP for host side of the bridge IP_NS=\"10.200.1.2/24\" # IP for namespace side # --- Cleanup any previous setup --- cleanup() { echo \"Cleaning up any previous setup...\" ip netns del \"$NS_NAME\" 2\u003e/dev/null || true ip link del \"$VETH_HOST\" 2\u003e/dev/null || true ip link del \"$BR_NAME\" 2\u003e/dev/null || true } cleanup # --- Create namespace --- echo \"Creating network namespace: $NS_NAME\" ip netns add \"$NS_NAME\" # --- Create veth pair --- echo \"Creating veth pair: $VETH_HOST \u003c-\u003e $VETH_NS\" ip link add \"$VETH_HOST\" type veth peer name \"$VETH_NS\" # --- Move one end into the namespace --- ip link set \"$VETH_NS\" netns \"$NS_NAME\" # --- Create and configure bridge --- echo \"Creating bridge: $BR_NAME\" ip link add name \"$BR_NAME\" type bridge ip addr add \"$IP_HOST\" dev \"$BR_NAME\" ip link set \"$BR_NAME\" up # --- Attach host veth to the bridge --- ip link set \"$VETH_HOST\" master \"$BR_NAME\" ip link set \"$VETH_HOST\" up # --- Configure namespace side --- ip netns exec \"$NS_NAME\" ip addr add \"$IP_NS\" dev \"$VETH_NS\" ip netns exec \"$NS_NAME\" ip link set \"$VETH_NS\" up ip netns exec \"$NS_NAME\" ip link set lo up ip netns exec \"$NS_NAME\" ip route add default via \"${IP_HOST%/*}\" # --- Enable IP forwarding (optional) --- echo \"Enabling IP forwarding...\" sysctl -w net.ipv4.ip_forward=1 \u003e/dev/null echo echo \"✅ Setup complete!\" echo \"Namespace: $NS_NAME\" echo \"Bridge: $BR_NAME ($IP_HOST)\" echo \"veth pair: $VETH_HOST \u003c-\u003e $VETH_NS\" echo echo \"To test connectivity:\" echo \" ip netns exec $NS_NAME ping -c 3 ${IP_HOST%/*}\" echo \"To enter the namespace shell:\" echo \" ip netns exec $NS_NAME bash\" To setup an ENI do something like this:\n#!/bin/bash # Configure an AWS Elastic Network Interface (ENI) # using 'ip link set', 'ip address add', and 'ip link set up'. set -euo pipefail # --- User configuration --- # Change these to match your setup ENI_IFACE=\"eth1\" # The interface name of the ENI (check with `ip link`) ENI_IP=\"10.0.2.50/24\" # Private IP address for the ENI ENI_GW=\"10.0.2.1\" # Default gateway (in ENI's subnet) ROUTE_TABLE=\"main\" # Optional: routing table to use # --- Validate interface existence --- if ! ip link show \"$ENI_IFACE\" \u0026\u003e/dev/null; then echo \"❌ Error: Interface $ENI_IFACE not found. Check 'ip link' output.\" exit 1 fi echo \"🔧 Configuring ENI interface: $ENI_IFACE\" # --- Bring interface down before configuring (optional safety) --- ip link set \"$ENI_IFACE\" down # --- Assign IP address --- echo \"➡️ Assigning IP address: $ENI_IP\" ip address flush dev \"$ENI_IFACE\" ip address add \"$ENI_IP\" dev \"$ENI_IFACE\" # --- Bring interface up --- echo \"⬆️ Bringing interface up...\" ip link set \"$ENI_IFACE\" up # --- Add default route (optional) --- echo \"🛣️ Setting default route via $ENI_GW\" ip route replace default via \"$ENI_GW\" dev \"$ENI_IFACE\" table \"$ROUTE_TABLE\" ip rule add from \"${ENI_IP%/*}\" lookup \"$ROUTE_TABLE\" || true echo echo \"✅ ENI configuration complete!\" echo \"Interface: $ENI_IFACE\" echo \"IP: $ENI_IP\" echo \"Gateway: $ENI_GW\" echo echo \"To verify:\" echo \" ip addr show dev $ENI_IFACE\" echo \" ip route show table $ROUTE_TABLE\" CNI Plugins Luckily, we don’t have to use IP tools (I believe we used to before CNI plugins) since containerD relies on external plugins for network configuration. Unlike Docker users, containerD users have the freedom to customize the networking environment.\nThe CNI plugins and the netlib package in the ecs-agent shared library encapsulate the complexity of setting up network namespaces. The caller makes two method calls: one to build the namespace configuration and one to “Start” the namespace. The high level sequence of events from Start goes something like this:\nCreate Namespace Configure DNS Configure ENI Create Bridge Get IP from IPAM Setup routes with iptable entries Configure Network Interface Move to netns Assign IPs Configure iptables and routes The plugins mainly wrap IP commands from the netlink library. This Go library simplifies adding and removing interfaces. Netlink is the successor of ioctl, a set of system calls historically (although still supported in some distros) used to configure serial devices. Since network interfaces are usually ethernet (at least in EC2 instances) this is the way to programmatically setup the system. Our customers shouldn’t have to care/worry since this is all done for them by the ECS dataplane.\nSo which CNIs do we use? For setting up awsvpc namespaces without any special networking features (such as Service Connect, AppMesh, Multi-ENI) we use the following:\nENI and Branch-ENI\nECS MI and Fargate use two types of ENIs: x-ENIs (Regular ENIs in the code) and branch ENIs (ENIs that leverage trunking). They are in the ENI and branch-eni packages respectively. Under the hood the plugins call netlib to setup the links.\nBridge and IPAM\nIn order to configure communication with the task metadata service (TMDS) we use IPAM to get an IP from the ECS subnet and Bridge to create a veth pair to bridge the namespaces.\nThis deep dive into ECS networking shows the sophisticated engineering that makes container isolation possible. Network namespaces provide the foundation for secure, isolated networking while CNI plugins abstract away the complexity for developers. Understanding these internals helps appreciate the elegant solutions that power modern container orchestration.\n","wordCount":"2073","inLanguage":"en","datePublished":"2025-10-12T17:12:00-07:00","dateModified":"2025-10-12T17:12:00-07:00","author":{"@type":"Person","name":"Jose Villalta"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://JoseVillalta.github.io/posts/network-namespaces-ecs-containers/"},"publisher":{"@type":"Organization","name":"Yes Way Jose","logo":{"@type":"ImageObject","url":"https://JoseVillalta.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JoseVillalta.github.io/ accesskey=h title="Yes Way Jose (Alt + H)">Yes Way Jose</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></span></div><ul id=menu><li><a href=https://JoseVillalta.github.io/experience/ title=CV><span>CV</span></a></li><li><a href=https://JoseVillalta.github.io/about/ title="About Me"><span>About Me</span></a></li><li><a href=http://joseavillalta.blogspot.com/ title="Personal Blog"><span>Personal Blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>A Deep Dive into Network Namespaces in AWS ECS Containers</h1><div class=post-description>Ever wondered what happens under the hood when you launch an ECS task with awsvpc networking? Let's explore the fascinating world of network isolation in AWS ECS.</div><div class=post-meta>&lt;span title='2025-10-12 17:12:00 -0700 -0700'>October 12, 2025&lt;/span>&amp;nbsp;·&amp;nbsp;Jose Villalta</div></header><div class=post-content><p>How do multiple containers run on the same EC2 instance without stepping on each other&rsquo;s network traffic? The answer lies in Linux network namespaces—a powerful isolation mechanism that gives each ECS task its own private network stack. Join me as we dissect a live ECS instance and uncover the elegant engineering that makes awsvpc networking possible.</p><h2 id=what-are-network-namespaces-for>What are network namespaces for?<a hidden class=anchor aria-hidden=true href=#what-are-network-namespaces-for>#</a></h2><p>A Linux namespace is a construct that creates an isolated copy of the networking stack, enabling them to run on the same host with different IP addresses, DNS configurations, and route tables. This allows multiple containers to run on the same machine without interfering with each other&rsquo;s network traffic.</p><p><img loading=lazy src=/img/v2-SingleBridge-Page-4.drawio.png alt="Network Namespaces Overview"></p><p>In ECS, when you want to run containerized applications, you create a task that can contain up to 10 containers. All containers within a task share the same network namespace. Each managed instance supports multiple network interfaces (ENIs) attached, task ENIS are provisioned by ECS Control plane at task launch time.</p><p>Let&rsquo;s take a look under the hood.</p><h2 id=setting-up-the-investigation>Setting Up the Investigation<a hidden class=anchor aria-hidden=true href=#setting-up-the-investigation>#</a></h2><p>I will launch an EC2 instance using an AMI provisioned with the ECS Managed Instance Agent running on Bottlerocket. This agent runs the same dataplane software that powers production instances. To enable debugging access, I&rsquo;ll create a variant that includes the login and SSM packages, allowing me to connect via the EC2 Serial Console. The instance will launch in EC2 debug mode, which means it won&rsquo;t be managed by the ECS Control Plane but will be fully owned by my account.</p><p>The AWS Console shows that I have an EC2 instance with two ENIs attached. I have two private IPs associated with each ENI. <code>10.194.20.168</code> is my task IP and <code>10.184.20.158</code> is the IP address of my host. The primary ENI is listed as Index 0 and the task ENI is listed as Index 1.</p><p><img loading=lazy src=/img/NetworkInterface.png alt="Network Interface Screenshot"></p><h2 id=examining-network-interfaces-from-the-host>Examining Network Interfaces from the Host<a hidden class=anchor aria-hidden=true href=#examining-network-interfaces-from-the-host>#</a></h2><p>Connecting to the instance as the root user and running <code>ip link show</code> produces the following output:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ip link show
</span></span><span style=display:flex><span>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>65536</span> qdisc noqueue state UNKNOWN mode DEFAULT group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span></span><span style=display:flex><span>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>9001</span> qdisc mq state UP mode DEFAULT group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/ether 02:df:6a:de:12:21 brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>    altname enp0s5
</span></span><span style=display:flex><span>    altname ens5
</span></span><span style=display:flex><span>4: fargate-bridge: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>1500</span> qdisc noqueue state UP mode DEFAULT group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/ether 2a:56:f8:7a:ea:8e brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>5: veth752433c6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>1500</span> qdisc noqueue master fargate-bridge state UP mode DEFAULT group default 
</span></span><span style=display:flex><span>    link/ether a2:03:66:23:29:82 brd ff:ff:ff:ff:ff:ff link-netns a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29
</span></span></code></pre></div><p>The <code>ip link show</code> command gives you the list of network interfaces in a Linux system. Notice that it only shows one ENI &ldquo;eth0&rdquo; but you don&rsquo;t see the task ENI &ldquo;eth1&rdquo;.</p><p>Let&rsquo;s confirm that eth0 is my actual primary ENI by comparing its address with the info from my AWS Console:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>bash-5.2# ip address show
</span></span><span style=display:flex><span>1: ...
</span></span><span style=display:flex><span>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>9001</span> qdisc mq state UP group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/ether 02:df:6a:de:12:21 brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>    altname enp0s5
</span></span><span style=display:flex><span>    altname ens5
</span></span><span style=display:flex><span>    inet 10.194.20.158/24 metric <span style=color:#ae81ff>1024</span> brd 10.194.20.255 scope global eth0
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>    inet6 fe80::df:6aff:fede:1221/64 scope link proto kernel_ll 
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>       ...
</span></span></code></pre></div><p>Yep, I see that the IP for eth0 is <code>10.194.20.158/24</code> and I see the MAC address matches. I also noticed that eth1 is not visible from the primary network namespace—as far as the host is concerned there is only one gateway to the internet.</p><h2 id=exploring-network-namespaces>Exploring Network Namespaces<a hidden class=anchor aria-hidden=true href=#exploring-network-namespaces>#</a></h2><p>Let&rsquo;s get a list of the network namespaces running:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>bash-5.2# ip netns show
</span></span><span style=display:flex><span>a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 <span style=color:#f92672>(</span>id: 0<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>bash-5.2# ctr -n fargate.task t ls
</span></span><span style=display:flex><span>TASK                                           PID     STATUS    
</span></span><span style=display:flex><span>a333f40b6ac74e92b1541fb0a5d76f9e-0883211837    <span style=color:#ae81ff>1551</span>    RUNNING
</span></span><span style=display:flex><span>bash-5.2# 
</span></span></code></pre></div><p>The first command <code>ip netns show</code> gives me a list of all the network namespaces. The second command is <code>ctr</code>, a command line client to talk to the containerd daemon. <code>t ls</code> lists all the tasks running in the fargate.task namespace.</p><p>Yes, we&rsquo;re running the Fargate Agent under the hood :)</p><p>Let&rsquo;s see what the namespace looks like from inside the network namespace (netns) of my awsvpc task. I&rsquo;ll use a nifty tool called <code>ip netns exec</code>, which allows me to launch commands inside a network namespace.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># ip netns exec a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 ip link show</span>
</span></span><span style=display:flex><span>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>65536</span> qdisc noqueue state UNKNOWN mode DEFAULT group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span></span><span style=display:flex><span>3: eth0@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>1500</span> qdisc noqueue state UP mode DEFAULT group default 
</span></span><span style=display:flex><span>    link/ether 0a:58:a9:fe:ac:02 brd ff:ff:ff:ff:ff:ff link-netnsid <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>4: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>9001</span> qdisc mq state UP mode DEFAULT group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/ether 02:ed:2d:97:4f:29 brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>    altname enp0s6
</span></span><span style=display:flex><span>    altname ens6
</span></span></code></pre></div><p>Well, there it is. <code>eth1</code> is here! Notice that eth0 is <strong>not</strong> here. As far as the container namespace is concerned, there&rsquo;s only one ENI in this host.</p><p>Let&rsquo;s confirm by looking at the IP address assigned to <code>eth1</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># ip netns exec a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 ip address show</span>
</span></span><span style=display:flex><span>1:...
</span></span><span style=display:flex><span>3:...
</span></span><span style=display:flex><span>4: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>9001</span> qdisc mq state UP group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/ether 02:ed:2d:97:4f:29 brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>    altname enp0s6
</span></span><span style=display:flex><span>    altname ens6
</span></span><span style=display:flex><span>    inet 10.194.20.168/24 scope global eth1
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>    inet6 fe80::ed:2dff:fe97:4f29/64 scope link proto kernel_ll 
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span></code></pre></div><p>Yes, eth1 has <code>10.194.20.168</code> which matches what the AWS console tells me.</p><h2 id=the-complete-network-picture>The Complete Network Picture<a hidden class=anchor aria-hidden=true href=#the-complete-network-picture>#</a></h2><p>Okay, but what about the other stuff I see in the IP command output?</p><p>Yes, good question. This is not obvious, but there&rsquo;s one more thing: there&rsquo;s a bridge from the task namespace that connects it to the task metadata service (TMDS). The namespace is connected via a veth pair. The full namespace picture looks like this:</p><p><img loading=lazy src=/img/v2-SingleBridge-Page-4.drawio-complte.png alt="Complete Network Namespace Diagram"></p><p>Now we see that <code>eth0@if5</code> is a veth (virtual ethernet) device connected to another veth in the primary interface that will route MAC frames to the fargate-bridge which acts as a virtual switch and will route MAC traffic to the task metadata server (TMDS) running on my primary namespace.</p><h2 id=what-else-goes-into-a-netns>What else goes into a netns?<a hidden class=anchor aria-hidden=true href=#what-else-goes-into-a-netns>#</a></h2><p>Besides the network interface we need to configure the namespace with everything it needs to work as an effectual copy of a Linux network stack, with its own routes, firewall rules and devices. In awsvpc tasks this means you get:</p><ul><li>A loopback interface</li><li>DNS configuration files</li><li>Routes in the iptables</li><li>A veth interface that connects to the primary netns</li></ul><h2 id=how-to-configure-network-namespaces-using-ip-commands>How to configure network namespaces using IP commands<a hidden class=anchor aria-hidden=true href=#how-to-configure-network-namespaces-using-ip-commands>#</a></h2><p>One way to configure network namespaces is to use the <a href=https://man7.org/linux/man-pages/man8/ip-netns.8.html>IP command</a> to create and configure the network namespace.</p><p>To configure a namespace like awsvpc, you need to create the netns and configure the ENI, you need to configure DNS and create a bridge from the netns to the task metadata server.</p><p>To create a netns use <code>ip netns add</code> command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo ip netns add my_container_ns
</span></span></code></pre></div><p>This command creates a new, empty network namespace named my_container_ns. It will appear as a mount point under /var/run/netns. <a href=https://lwn.net/Articles/580893/>[1]</a></p><p>If you want to configure an ENI you&rsquo;d use <code>ip link</code> commands to create, setup, add IP address, set MTU, etc. For illustrative purposes I&rsquo;ve added some scripts created with generative AI.</p><p>This looks about right (use at your own risk, this is untested code).</p><p>To create a namespace with a bridge run something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e># This script creates a network namespace, a veth pair, and a bridge between</span>
</span></span><span style=display:flex><span><span style=color:#75715e># the host and the network namespace.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>set -e
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Configuration ---</span>
</span></span><span style=display:flex><span>NS_NAME<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;testns&#34;</span>         <span style=color:#75715e># Name of the network namespace</span>
</span></span><span style=display:flex><span>VETH_HOST<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;veth-host&#34;</span>    <span style=color:#75715e># veth interface in the root namespace</span>
</span></span><span style=display:flex><span>VETH_NS<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;veth-ns&#34;</span>        <span style=color:#75715e># veth interface inside the namespace</span>
</span></span><span style=display:flex><span>BR_NAME<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;br0&#34;</span>            <span style=color:#75715e># Name of the bridge</span>
</span></span><span style=display:flex><span>IP_HOST<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;10.200.1.1/24&#34;</span>  <span style=color:#75715e># IP for host side of the bridge</span>
</span></span><span style=display:flex><span>IP_NS<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;10.200.1.2/24&#34;</span>    <span style=color:#75715e># IP for namespace side</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Cleanup any previous setup ---</span>
</span></span><span style=display:flex><span>cleanup<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    echo <span style=color:#e6db74>&#34;Cleaning up any previous setup...&#34;</span>
</span></span><span style=display:flex><span>    ip netns del <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span> 2&gt;/dev/null <span style=color:#f92672>||</span> true
</span></span><span style=display:flex><span>    ip link del <span style=color:#e6db74>&#34;</span>$VETH_HOST<span style=color:#e6db74>&#34;</span> 2&gt;/dev/null <span style=color:#f92672>||</span> true
</span></span><span style=display:flex><span>    ip link del <span style=color:#e6db74>&#34;</span>$BR_NAME<span style=color:#e6db74>&#34;</span> 2&gt;/dev/null <span style=color:#f92672>||</span> true
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>cleanup
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Create namespace ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Creating network namespace: </span>$NS_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip netns add <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Create veth pair ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Creating veth pair: </span>$VETH_HOST<span style=color:#e6db74> &lt;-&gt; </span>$VETH_NS<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip link add <span style=color:#e6db74>&#34;</span>$VETH_HOST<span style=color:#e6db74>&#34;</span> type veth peer name <span style=color:#e6db74>&#34;</span>$VETH_NS<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Move one end into the namespace ---</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$VETH_NS<span style=color:#e6db74>&#34;</span> netns <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Create and configure bridge ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Creating bridge: </span>$BR_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip link add name <span style=color:#e6db74>&#34;</span>$BR_NAME<span style=color:#e6db74>&#34;</span> type bridge
</span></span><span style=display:flex><span>ip addr add <span style=color:#e6db74>&#34;</span>$IP_HOST<span style=color:#e6db74>&#34;</span> dev <span style=color:#e6db74>&#34;</span>$BR_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$BR_NAME<span style=color:#e6db74>&#34;</span> up
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Attach host veth to the bridge ---</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$VETH_HOST<span style=color:#e6db74>&#34;</span> master <span style=color:#e6db74>&#34;</span>$BR_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$VETH_HOST<span style=color:#e6db74>&#34;</span> up
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Configure namespace side ---</span>
</span></span><span style=display:flex><span>ip netns exec <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span> ip addr add <span style=color:#e6db74>&#34;</span>$IP_NS<span style=color:#e6db74>&#34;</span> dev <span style=color:#e6db74>&#34;</span>$VETH_NS<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip netns exec <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span> ip link set <span style=color:#e6db74>&#34;</span>$VETH_NS<span style=color:#e6db74>&#34;</span> up
</span></span><span style=display:flex><span>ip netns exec <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span> ip link set lo up
</span></span><span style=display:flex><span>ip netns exec <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span> ip route add default via <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>IP_HOST%/*<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Enable IP forwarding (optional) ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Enabling IP forwarding...&#34;</span>
</span></span><span style=display:flex><span>sysctl -w net.ipv4.ip_forward<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> &gt;/dev/null
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;✅ Setup complete!&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Namespace: </span>$NS_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Bridge:    </span>$BR_NAME<span style=color:#e6db74> (</span>$IP_HOST<span style=color:#e6db74>)&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;veth pair: </span>$VETH_HOST<span style=color:#e6db74> &lt;-&gt; </span>$VETH_NS<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;To test connectivity:&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;  ip netns exec </span>$NS_NAME<span style=color:#e6db74> ping -c 3 </span><span style=color:#e6db74>${</span>IP_HOST%/*<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;To enter the namespace shell:&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;  ip netns exec </span>$NS_NAME<span style=color:#e6db74> bash&#34;</span>
</span></span></code></pre></div><p>To setup an ENI do something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e># Configure an AWS Elastic Network Interface (ENI)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># using &#39;ip link set&#39;, &#39;ip address add&#39;, and &#39;ip link set up&#39;.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>set -euo pipefail
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- User configuration ---</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Change these to match your setup</span>
</span></span><span style=display:flex><span>ENI_IFACE<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;eth1&#34;</span>           <span style=color:#75715e># The interface name of the ENI (check with `ip link`)</span>
</span></span><span style=display:flex><span>ENI_IP<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;10.0.2.50/24&#34;</span>      <span style=color:#75715e># Private IP address for the ENI</span>
</span></span><span style=display:flex><span>ENI_GW<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;10.0.2.1&#34;</span>          <span style=color:#75715e># Default gateway (in ENI&#39;s subnet)</span>
</span></span><span style=display:flex><span>ROUTE_TABLE<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;main&#34;</span>         <span style=color:#75715e># Optional: routing table to use</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Validate interface existence ---</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> ! ip link show <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span> &amp;&gt;/dev/null; <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>    echo <span style=color:#e6db74>&#34;❌ Error: Interface </span>$ENI_IFACE<span style=color:#e6db74> not found. Check &#39;ip link&#39; output.&#34;</span>
</span></span><span style=display:flex><span>    exit <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>fi</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;🔧 Configuring ENI interface: </span>$ENI_IFACE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Bring interface down before configuring (optional safety) ---</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span> down
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Assign IP address ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;➡️  Assigning IP address: </span>$ENI_IP<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip address flush dev <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip address add <span style=color:#e6db74>&#34;</span>$ENI_IP<span style=color:#e6db74>&#34;</span> dev <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Bring interface up ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;⬆️  Bringing interface up...&#34;</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span> up
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Add default route (optional) ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;🛣️  Setting default route via </span>$ENI_GW<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip route replace default via <span style=color:#e6db74>&#34;</span>$ENI_GW<span style=color:#e6db74>&#34;</span> dev <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span> table <span style=color:#e6db74>&#34;</span>$ROUTE_TABLE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip rule add from <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>ENI_IP%/*<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> lookup <span style=color:#e6db74>&#34;</span>$ROUTE_TABLE<span style=color:#e6db74>&#34;</span> <span style=color:#f92672>||</span> true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;✅ ENI configuration complete!&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Interface: </span>$ENI_IFACE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;IP:        </span>$ENI_IP<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Gateway:   </span>$ENI_GW<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;To verify:&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;  ip addr show dev </span>$ENI_IFACE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;  ip route show table </span>$ROUTE_TABLE<span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div><h2 id=cni-plugins>CNI Plugins<a hidden class=anchor aria-hidden=true href=#cni-plugins>#</a></h2><p>Luckily, we don&rsquo;t have to use IP tools (I believe we used to before CNI plugins) since containerD relies on external plugins for network configuration. Unlike Docker users, containerD users have the freedom to customize the networking environment.</p><p>The CNI plugins and the netlib package in the ecs-agent shared library encapsulate the complexity of setting up network namespaces. The caller makes two method calls: one to build the namespace configuration and one to &ldquo;Start&rdquo; the namespace. The high level sequence of events from Start goes something like this:</p><p><img loading=lazy src=/img/cni-seq.png alt="CNI Workflow Diagram"></p><ul><li><strong>Create Namespace</strong><ul><li>Configure DNS</li></ul></li><li><strong>Configure ENI</strong><ul><li><strong>Create Bridge</strong><ul><li>Get IP from IPAM</li><li>Setup routes with iptable entries</li></ul></li><li><strong>Configure Network Interface</strong><ul><li>Move to netns</li><li>Assign IPs</li><li>Configure iptables and routes</li></ul></li></ul></li></ul><p>The plugins mainly wrap IP commands from the netlink library. This Go library simplifies adding and removing interfaces. Netlink is the successor of ioctl, a set of system calls historically (although still supported in some distros) used to configure serial devices. Since network interfaces are usually ethernet (at least in EC2 instances) this is the way to programmatically setup the system. Our customers shouldn&rsquo;t have to care/worry since this is all done for them by the ECS dataplane.</p><h2 id=so-which-cnis-do-we-use>So which CNIs do we use?<a hidden class=anchor aria-hidden=true href=#so-which-cnis-do-we-use>#</a></h2><p>For setting up awsvpc namespaces without any special networking features (such as Service Connect, AppMesh, Multi-ENI) we use the following:</p><p><strong>ENI and Branch-ENI</strong></p><p>ECS MI and Fargate use two types of ENIs: x-ENIs (Regular ENIs in the code) and branch ENIs (ENIs that leverage trunking). They are in the ENI and branch-eni packages respectively. Under the hood the plugins call netlib to setup the links.</p><p><strong>Bridge and IPAM</strong></p><p>In order to configure communication with the task metadata service (TMDS) we use IPAM to get an IP from the ECS subnet and Bridge to create a veth pair to bridge the namespaces.</p><hr><p><em>This deep dive into ECS networking shows the sophisticated engineering that makes container isolation possible. Network namespaces provide the foundation for secure, isolated networking while CNI plugins abstract away the complexity for developers. Understanding these internals helps appreciate the elegant solutions that power modern container orchestration.</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://JoseVillalta.github.io/tags/aws/>Aws</a></li><li><a href=https://JoseVillalta.github.io/tags/ecs/>Ecs</a></li><li><a href=https://JoseVillalta.github.io/tags/containers/>Containers</a></li><li><a href=https://JoseVillalta.github.io/tags/networking/>Networking</a></li><li><a href=https://JoseVillalta.github.io/tags/linux/>Linux</a></li><li><a href=https://JoseVillalta.github.io/tags/namespaces/>Namespaces</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://JoseVillalta.github.io/>Yes Way Jose</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>