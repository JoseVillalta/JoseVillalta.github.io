<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Deep Dive into Network Namespaces in AWS ECS Containers | Yes Way Jose</title><meta name=keywords content="aws,ecs,containers,networking,linux,namespaces"><meta name=description content="Ever wondered what happens under the hood when you launch an ECS task with awsvpc networking? Let's explore how network namespaces are put together when you run containers in ECS Managed Instances."><meta name=author content="Jose Villalta"><link rel=canonical href=https://JoseVillalta.github.io/posts/network-namespaces-ecs-containers/><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.555af97124d54bb1457985dd081b8f5616a48103aafeb30ac89fde835d65aa6c.js integrity="sha256-VVr5cSTVS7FFeYXdCBuPVhakgQOq/rMKyJ/eg11lqmw=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://JoseVillalta.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://JoseVillalta.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://JoseVillalta.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://JoseVillalta.github.io/apple-touch-icon.png><link rel=mask-icon href=https://JoseVillalta.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://JoseVillalta.github.io/posts/network-namespaces-ecs-containers/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="A Deep Dive into Network Namespaces in AWS ECS Containers"><meta property="og:description" content="Ever wondered what happens under the hood when you launch an ECS task with awsvpc networking? Let's explore how network namespaces are put together when you run containers in ECS Managed Instances."><meta property="og:type" content="article"><meta property="og:url" content="https://JoseVillalta.github.io/posts/network-namespaces-ecs-containers/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-12T17:12:00-07:00"><meta property="article:modified_time" content="2025-10-12T17:12:00-07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Deep Dive into Network Namespaces in AWS ECS Containers"><meta name=twitter:description content="Ever wondered what happens under the hood when you launch an ECS task with awsvpc networking? Let's explore how network namespaces are put together when you run containers in ECS Managed Instances."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://JoseVillalta.github.io/posts/"},{"@type":"ListItem","position":2,"name":"A Deep Dive into Network Namespaces in AWS ECS Containers","item":"https://JoseVillalta.github.io/posts/network-namespaces-ecs-containers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Deep Dive into Network Namespaces in AWS ECS Containers","name":"A Deep Dive into Network Namespaces in AWS ECS Containers","description":"Ever wondered what happens under the hood when you launch an ECS task with awsvpc networking? Let's explore how network namespaces are put together when you run containers in ECS Managed Instances.","keywords":["aws","ecs","containers","networking","linux","namespaces"],"articleBody":"What goes into a network namespace? What’s a CNI plugin? This blog post explores the contents of a network namespace and then provides links to the open source code that creates and configures namespaces for containarized workloads running on Fargate and ECS Managed Instances.\nBackground and Motivation When you launch a task in ECS Managed Instances you can pick two network modes, awsvpc and host. In this series of post I want to explain the role in the CNI plugins have in creating and setting up network namespaces for customer tasks. In awsvpc each task receives its own elastic network interface (ENI) and private IPv4 address. Under the hood the code that creates and manages network namespaces (netns) for Fargate also handles Managed Instances. The plugins that handles this setup are open source but they are somewhat hidden behind the netlib platform API (as they should) so the typical dataplane engineer does not see the plugin code on day-to-day therefore i’s’ kind of a black box to most folks in my team, but it doesn’t have to be.\nWhat are network namespaces for? A Linux namespace is a construct that creates an isolated copy of the networking stack. Namespaces allows multiple ECS tasks to run on the same host with different IP addresses, DNS configurations, and route tables.\nIn ECS, when you want to run containerized applications, you create a task that can contain up to 10 containers. All containers within a task share the same network namespace. Each managed instance supports multiple network interfaces (ENIs) attached, task ENIS are provisioned by ECS Control plane at task launch time.\nLet’s take a look under the hood.\nSetting Up the Investigation I will launch an EC2 instance using an AMI provisioned with the ECS Managed Instance Agent running on Bottlerocket. This agent runs the same dataplane software that powers production instances. To enable debugging access, I’ll create a variant that includes the login and SSM packages, allowing me to connect via the EC2 Serial Console. The instance will launch in EC2 debug mode, which means it won’t be managed by the ECS Control Plane but will be fully owned by my account.\nThe AWS Console shows that I have an EC2 instance with two ENIs attached. I have two private IPs associated with each ENI. 10.194.20.168 is my task IP and 10.184.20.158 is the IP address of my host. The primary ENI is listed as Index 0 and the task ENI is listed as Index 1.\nExamining Network Interfaces from the Host Connecting to the instance as the root user and running ip link show produces the following output:\nip link show 1: lo: mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 02:df:6a:de:12:21 brd ff:ff:ff:ff:ff:ff altname enp0s5 altname ens5 4: fargate-bridge: mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 2a:56:f8:7a:ea:8e brd ff:ff:ff:ff:ff:ff 5: veth752433c6@if3: mtu 1500 qdisc noqueue master fargate-bridge state UP mode DEFAULT group default link/ether a2:03:66:23:29:82 brd ff:ff:ff:ff:ff:ff link-netns a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 The ip link show command gives you the list of network interfaces in a Linux system. Notice that it only shows one ENI “eth0” but you don’t see the task ENI “eth1”.\nLet’s confirm that eth0 is my actual primary ENI by comparing its address with the info from my AWS Console:\nbash-5.2# ip address show 1: ... 2: eth0: mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 02:df:6a:de:12:21 brd ff:ff:ff:ff:ff:ff altname enp0s5 altname ens5 inet 10.194.20.158/24 metric 1024 brd 10.194.20.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::df:6aff:fede:1221/64 scope link proto kernel_ll valid_lft forever preferred_lft forever ... Yep, I see that the IP for eth0 is 10.194.20.158/24 and I see the MAC address matches. I also noticed that eth1 is not visible from the primary network namespace—as far as the host is concerned there is only one gateway to the internet.\nExploring Network Namespaces Let’s get a list of the network namespaces running:\nbash-5.2# ip netns show a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 (id: 0) bash-5.2# ctr -n fargate.task t ls TASK PID STATUS a333f40b6ac74e92b1541fb0a5d76f9e-0883211837 1551 RUNNING bash-5.2# The first command ip netns show gives me a list of all the network namespaces. The second command is ctr, a command line client to talk to the containerd daemon. t ls lists all the tasks running in the fargate.task namespace.\nYes, we’re running the Fargate Agent under the hood :)\nLet’s see what the namespace looks like from inside the network namespace (netns) of my awsvpc task. I’ll use a nifty tool called ip netns exec, which allows me to launch commands inside a network namespace.\n# ip netns exec a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 ip link show 1: lo: mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 3: eth0@if5: mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 0a:58:a9:fe:ac:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 4: eth1: mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 02:ed:2d:97:4f:29 brd ff:ff:ff:ff:ff:ff altname enp0s6 altname ens6 Well, there it is. eth1 is here! Notice that eth0 is not here. As far as the container namespace is concerned, there’s only one ENI in this host.\nLet’s confirm by looking at the IP address assigned to eth1:\n# ip netns exec a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 ip address show 1:... 3:... 4: eth1: mtu 9001 qdisc mq state UP group default qlen 1000 link/ether 02:ed:2d:97:4f:29 brd ff:ff:ff:ff:ff:ff altname enp0s6 altname ens6 inet 10.194.20.168/24 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::ed:2dff:fe97:4f29/64 scope link proto kernel_ll valid_lft forever preferred_lft forever Yes, eth1 has 10.194.20.168 which matches what the AWS console tells me.\nThe Complete Network Picture Okay, but what about the other stuff I see in the IP command output?\nYes, good question. This is not obvious, but there’s one more thing: there’s a bridge from the task namespace that connects it to the task metadata service (TMDS). The namespace is connected via a veth pair. The full namespace picture looks like this:\nNow we see that eth0@if5 is a veth (virtual ethernet) device connected to another veth in the primary interface that will route MAC frames to the fargate-bridge which acts as a virtual switch and will route MAC traffic to the task metadata server (TMDS) running on my primary namespace.\nWhat else goes into a netns? Besides the network interface we need to configure the namespace with everything it needs to work as an effectual copy of a Linux network stack, with its own routes, firewall rules and devices. In awsvpc tasks this means you get:\nA loopback interface DNS configuration files Routes in the iptables A veth interface that connects to the primary netns How to configure network namespaces using IP commands One way to configure network namespaces is to use the IP command to create and configure the network namespace.\nTo configure a namespace like awsvpc, you need to create the netns and configure the ENI, you need to configure DNS and create a bridge from the netns to the task metadata server.\nTo create a netns use ip netns add command:\nsudo ip netns add my_container_ns This command creates a new, empty network namespace named my_container_ns. It will appear as a mount point under /var/run/netns. [1]\nIf you want to configure an ENI you’d use ip link commands to create, setup, add IP address, set MTU, etc. For illustrative purposes I’ve added some scripts created with generative AI.\nThis looks about right (use at your own risk, this is untested code).\nTo create a namespace with a bridge run something like this:\n#!/bin/bash # This script creates a network namespace, a veth pair, and a bridge between # the host and the network namespace. set -e # --- Configuration --- NS_NAME=\"testns\" # Name of the network namespace VETH_HOST=\"veth-host\" # veth interface in the root namespace VETH_NS=\"veth-ns\" # veth interface inside the namespace BR_NAME=\"br0\" # Name of the bridge IP_HOST=\"10.200.1.1/24\" # IP for host side of the bridge IP_NS=\"10.200.1.2/24\" # IP for namespace side # --- Cleanup any previous setup --- cleanup() { echo \"Cleaning up any previous setup...\" ip netns del \"$NS_NAME\" 2\u003e/dev/null || true ip link del \"$VETH_HOST\" 2\u003e/dev/null || true ip link del \"$BR_NAME\" 2\u003e/dev/null || true } cleanup # --- Create namespace --- echo \"Creating network namespace: $NS_NAME\" ip netns add \"$NS_NAME\" # --- Create veth pair --- echo \"Creating veth pair: $VETH_HOST \u003c-\u003e $VETH_NS\" ip link add \"$VETH_HOST\" type veth peer name \"$VETH_NS\" # --- Move one end into the namespace --- ip link set \"$VETH_NS\" netns \"$NS_NAME\" # --- Create and configure bridge --- echo \"Creating bridge: $BR_NAME\" ip link add name \"$BR_NAME\" type bridge ip addr add \"$IP_HOST\" dev \"$BR_NAME\" ip link set \"$BR_NAME\" up # --- Attach host veth to the bridge --- ip link set \"$VETH_HOST\" master \"$BR_NAME\" ip link set \"$VETH_HOST\" up # --- Configure namespace side --- ip netns exec \"$NS_NAME\" ip addr add \"$IP_NS\" dev \"$VETH_NS\" ip netns exec \"$NS_NAME\" ip link set \"$VETH_NS\" up ip netns exec \"$NS_NAME\" ip link set lo up ip netns exec \"$NS_NAME\" ip route add default via \"${IP_HOST%/*}\" # --- Enable IP forwarding (optional) --- echo \"Enabling IP forwarding...\" sysctl -w net.ipv4.ip_forward=1 \u003e/dev/null echo echo \"✅ Setup complete!\" echo \"Namespace: $NS_NAME\" echo \"Bridge: $BR_NAME ($IP_HOST)\" echo \"veth pair: $VETH_HOST \u003c-\u003e $VETH_NS\" echo echo \"To test connectivity:\" echo \" ip netns exec $NS_NAME ping -c 3 ${IP_HOST%/*}\" echo \"To enter the namespace shell:\" echo \" ip netns exec $NS_NAME bash\" To setup an ENI do something like this:\n#!/bin/bash # Configure an AWS Elastic Network Interface (ENI) # using 'ip link set', 'ip address add', and 'ip link set up'. set -euo pipefail # --- User configuration --- # Change these to match your setup ENI_IFACE=\"eth1\" # The interface name of the ENI (check with `ip link`) ENI_IP=\"10.0.2.50/24\" # Private IP address for the ENI ENI_GW=\"10.0.2.1\" # Default gateway (in ENI's subnet) ROUTE_TABLE=\"main\" # Optional: routing table to use # --- Validate interface existence --- if ! ip link show \"$ENI_IFACE\" \u0026\u003e/dev/null; then echo \"❌ Error: Interface $ENI_IFACE not found. Check 'ip link' output.\" exit 1 fi echo \"🔧 Configuring ENI interface: $ENI_IFACE\" # --- Bring interface down before configuring (optional safety) --- ip link set \"$ENI_IFACE\" down # --- Assign IP address --- echo \"➡️ Assigning IP address: $ENI_IP\" ip address flush dev \"$ENI_IFACE\" ip address add \"$ENI_IP\" dev \"$ENI_IFACE\" # --- Bring interface up --- echo \"⬆️ Bringing interface up...\" ip link set \"$ENI_IFACE\" up # --- Add default route (optional) --- echo \"🛣️ Setting default route via $ENI_GW\" ip route replace default via \"$ENI_GW\" dev \"$ENI_IFACE\" table \"$ROUTE_TABLE\" ip rule add from \"${ENI_IP%/*}\" lookup \"$ROUTE_TABLE\" || true echo echo \"✅ ENI configuration complete!\" echo \"Interface: $ENI_IFACE\" echo \"IP: $ENI_IP\" echo \"Gateway: $ENI_GW\" echo echo \"To verify:\" echo \" ip addr show dev $ENI_IFACE\" echo \" ip route show table $ROUTE_TABLE\" CNI Plugins Luckily, we don’t have to use IP tools (I believe we used to before CNI plugins) since containerD relies on external plugins for network configuration. Unlike Docker users, containerD users have the freedom to customize the networking environment.\nThe CNI plugins and the netlib package in the ecs-agent shared library encapsulate the complexity of setting up network namespaces. The caller makes two method calls: one to build the namespace configuration and one to “Start” the namespace. The high level sequence of events from Start goes something like this:\nCreate Namespace Configure DNS Configure ENI Create Bridge Get IP from IPAM Setup routes with iptable entries Configure Network Interface Move to netns Assign IPs Configure iptables and routes The plugins mainly wrap IP commands from the netlink library. This Go library simplifies adding and removing interfaces. Netlink is the successor of ioctl, a set of system calls historically (although still supported in some distros) used to configure serial devices. Since network interfaces are usually ethernet (at least in EC2 instances) this is the way to programmatically setup the system. Our customers shouldn’t have to care/worry since this is all done for them by the ECS dataplane.\nSo which CNIs do we use? For setting up awsvpc namespaces without any special networking features (such as Service Connect, AppMesh, Multi-ENI) we use the following:\nENI and Branch-ENI\nECS Managed Instances and Fargate use two types of ENIs: x-ENIs (Regular ENIs in the code) and branch ENIs (ENIs that leverage trunking). They are in the ENI and branch-eni packages respectively. Under the hood the plugins call netlib to setup the links.\nThe ECS ENI plugin configures a container’s network namespace to use an Elastic Network Interface (ENI) directly. It moves an ENI from the host into the container’s network namespace.\nKey Components Engine Interface The core logic is in the engine package, which handles:\nRetrieving ENI metadata from EC2 instance metadata service Finding the network device by MAC address Setting up the container namespace with the ENI Main Flow Plugin Entry: The main function uses CNI’s skel.PluginMain() to handle ADD/DEL commands\nADD Command Process:\nParses configuration (ENI ID, MAC address, IP addresses) Uses EC2 metadata service to find the ENI’s network device name Calls SetupContainerNamespace() to move the ENI into the container Key Engine Operations:\nGetInterfaceDeviceName(): Finds the host device name using the MAC address GetIPV4GatewayNetmask(): Retrieves gateway/netmask from EC2 metadata GetIPV6Gateway(): Gets IPv6 gateway from routing table SetupContainerNamespace(): Moves the ENI into container’s network namespace Configuration Parameters The plugin accepts:\neni: ENI ID (required) mac: MAC address (required) ipv4-address: Primary IPv4 address (required) ipv6-address: IPv6 address (optional) block-instance-metadata: Block metadata access (optional) stay-down: Keep interface down (optional) How It Works The plugin receives an ENI that’s already attached to the EC2 instance It uses the MAC address to find the corresponding network interface on the host It retrieves network configuration (gateway, netmask) from EC2 metadata service It moves the entire ENI device into the container’s network namespace Optionally starts DHCP client for lease renewal This gives containers direct access to ENIs with their own IP addresses, enabling advanced networking features like security groups per container.\nECS Bridge and IPAM Plugins ECS Bridge Plugin Overview The ECS Bridge plugin creates a bridge network to connect containers to the ECS Agent’s credentials endpoint. It creates a bridge device and veth pairs to enable communication between containers and the host.\nKey Components Main Flow:\nBridge Creation: Creates or reuses a bridge device (e.g., ecs-br0) Veth Pair Creation: Creates a virtual ethernet pair - one end in container namespace, one on host IPAM Integration: Calls the ECS IPAM plugin to allocate IP addresses Interface Configuration: Configures both ends of the veth pair with IP addresses and routes Key Operations:\nCreateBridge(): Creates the bridge device with specified MTU CreateVethPair(): Creates veth pair connecting container to host AttachHostVethInterfaceToBridge(): Connects host veth to bridge RunIPAMPluginAdd(): Calls IPAM plugin for IP allocation ConfigureContainerVethInterface(): Sets up container’s network interface ConfigureBridge(): Configures bridge with gateway IP Configuration Parameters bridge: Bridge name (required) ipam: IPAM configuration (required) mtu: Maximum transmission unit (optional) ECS IPAM Plugin Overview The ECS IPAM (IP Address Management) plugin allocates IP addresses from a specified subnet and manages IP address assignments using a BoltDB database for persistence.\nKey Components IP Management:\nUses BoltDB for persistent IP address tracking Allocates IPs from a configured subnet (e.g., 169.254.172.0/22) Tracks last known IP to optimize allocation Supports both automatic and manual IP assignment Main Operations:\nAdd(): Allocates an IP address and returns network configuration Del(): Releases an IP address back to the pool GetAvailableIP(): Finds next available IP in subnet Assign(): Marks specific IP as used Release(): Frees an IP address Configuration Parameters ipv4-subnet: CIDR block for allocations (required) ipv4-address: Specific IP to assign (optional) ipv4-gateway: Gateway IP (optional, defaults to .1) ipv4-routes: Routes to add to container (optional) id: Unique identifier for IP assignment (optional) How They Work Together Bridge Plugin Invocation: ECS Agent calls bridge plugin with configuration Bridge Setup: Plugin creates bridge device and veth pair IPAM Call: Bridge plugin calls IPAM plugin to get IP allocation IP Assignment: IPAM plugin allocates IP from subnet and stores in database Interface Configuration: Bridge plugin configures container interface with allocated IP Route Setup: Adds routes for ECS credentials endpoint communication This enables containers to communicate with the ECS Agent’s credentials endpoint at 169.254.170.2 while maintaining network isolation.\nConclusion Diving into ECS networking internals shows how sophisticated systems can appear simple from the outside. Network namespaces provide the isolation, CNI plugins manage the complexity, and developers get clean abstractions. Sometimes the best engineering is the kind you never have to think about.\nFurther resources ECS Documentation Networking for ECS Managed Instances\nECS Agent netlib library This is where the code that invokes the CNIs live: netlib\nCNI plugins source code This package contains the bridge, eni and IPAM plugins, which are the “bread and butter” of network configuration. They are used by both Fargate and Managed Instances: ecs-cni-plugins\nSpecial CNIs that are aws vpc specific. Managed Instances make use of the vpc-branch-eni plugin to configure trunk and branch ENIs. Trunking is an AWS feature that allows for more task density on hosts by giving the capability to pack more ENIs per instance.\nvpc-cni-plugins\nIP Command Source Code for iproute, the networking utility that powers the CNI plugins: iproute2\nNetlink Protocol netlink\nLinux Networking Resources Official documentation networking\nFree Online Books Computer Networks: A System Approach Software-Defined Networks: A Systems Approach\nMy favorite networking books Understanding Linux Network Internals: Guided Tour to Networking on Linux is a bit dated but still good source of background.\nI saw this book in Julia Evans bookshelf Julia is an excellent software engineer and gifted technical writer, I cannot recommend reading her blog highly enough. Networking for System Administrators\nAlright, this post is getting long, I’ll stop now, but come back for the next installation where I’ll go over what I diddn’t get a chance to get into.\nThis deep dive into ECS networking shows the sophisticated engineering that makes container isolation possible. Network namespaces provide the foundation for secure, isolated networking while CNI plugins abstract away the complexity for developers. Understanding these internals helps appreciate the elegant solutions that power modern container orchestration.\n","wordCount":"3039","inLanguage":"en","datePublished":"2025-10-12T17:12:00-07:00","dateModified":"2025-10-12T17:12:00-07:00","author":{"@type":"Person","name":"Jose Villalta"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://JoseVillalta.github.io/posts/network-namespaces-ecs-containers/"},"publisher":{"@type":"Organization","name":"Yes Way Jose","logo":{"@type":"ImageObject","url":"https://JoseVillalta.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://JoseVillalta.github.io/ accesskey=h title="Yes Way Jose (Alt + H)">Yes Way Jose</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></span></div><ul id=menu><li><a href=https://JoseVillalta.github.io/experience/ title=CV><span>CV</span></a></li><li><a href=https://JoseVillalta.github.io/about/ title="About Me"><span>About Me</span></a></li><li><a href=http://joseavillalta.blogspot.com/ title="Personal Blog"><span>Personal Blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>A Deep Dive into Network Namespaces in AWS ECS Containers</h1><div class=post-description>Ever wondered what happens under the hood when you launch an ECS task with awsvpc networking? Let's explore how network namespaces are put together when you run containers in ECS Managed Instances.</div><div class=post-meta>&lt;span title='2025-10-12 17:12:00 -0700 -0700'>October 12, 2025&lt;/span>&amp;nbsp;·&amp;nbsp;Jose Villalta</div></header><div class=post-content><p>What goes into a network namespace? What&rsquo;s a CNI plugin? This blog post explores the contents of a network namespace and then provides links to the open source code that creates and configures namespaces for containarized workloads running on Fargate and ECS Managed Instances.</p><h2 id=background-and-motivation>Background and Motivation<a hidden class=anchor aria-hidden=true href=#background-and-motivation>#</a></h2><p>When you launch a task in ECS Managed Instances you can pick two network modes, awsvpc and host. In this series of post I want to explain the role in the CNI plugins have in creating and setting up network namespaces for customer tasks. In awsvpc each task receives its own elastic network interface (ENI) and private IPv4 address. Under the hood the code that creates and manages network namespaces (netns) for Fargate also handles Managed Instances. The plugins that handles this setup are open source but they are somewhat hidden behind the netlib platform API (as they should) so the typical dataplane engineer does not see the plugin code on day-to-day therefore i&rsquo;s&rsquo; kind of a black box to most folks in my team, but it doesn&rsquo;t have to be.</p><h2 id=what-are-network-namespaces-for>What are network namespaces for?<a hidden class=anchor aria-hidden=true href=#what-are-network-namespaces-for>#</a></h2><p>A Linux namespace is a construct that creates an isolated copy of the networking stack. Namespaces allows multiple ECS tasks to run on the same host with different IP addresses, DNS configurations, and route tables.</p><p><img loading=lazy src=/img/v2-SingleBridge-Page-4.drawio.png alt="Network Namespaces Overview"></p><p>In ECS, when you want to run containerized applications, you create a task that can contain up to 10 containers. All containers within a task share the same network namespace. Each managed instance supports multiple network interfaces (ENIs) attached, task ENIS are provisioned by ECS Control plane at task launch time.</p><p>Let&rsquo;s take a look under the hood.</p><h2 id=setting-up-the-investigation>Setting Up the Investigation<a hidden class=anchor aria-hidden=true href=#setting-up-the-investigation>#</a></h2><p>I will launch an EC2 instance using an AMI provisioned with the ECS Managed Instance Agent running on Bottlerocket. This agent runs the same dataplane software that powers production instances. To enable debugging access, I&rsquo;ll create a variant that includes the login and SSM packages, allowing me to connect via the EC2 Serial Console. The instance will launch in EC2 debug mode, which means it won&rsquo;t be managed by the ECS Control Plane but will be fully owned by my account.</p><p>The AWS Console shows that I have an EC2 instance with two ENIs attached. I have two private IPs associated with each ENI. <code>10.194.20.168</code> is my task IP and <code>10.184.20.158</code> is the IP address of my host. The primary ENI is listed as Index 0 and the task ENI is listed as Index 1.</p><p><img loading=lazy src=/img/NetworkInterface.png alt="Network Interface Screenshot"></p><h2 id=examining-network-interfaces-from-the-host>Examining Network Interfaces from the Host<a hidden class=anchor aria-hidden=true href=#examining-network-interfaces-from-the-host>#</a></h2><p>Connecting to the instance as the root user and running <code>ip link show</code> produces the following output:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ip link show
</span></span><span style=display:flex><span>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>65536</span> qdisc noqueue state UNKNOWN mode DEFAULT group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span></span><span style=display:flex><span>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>9001</span> qdisc mq state UP mode DEFAULT group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/ether 02:df:6a:de:12:21 brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>    altname enp0s5
</span></span><span style=display:flex><span>    altname ens5
</span></span><span style=display:flex><span>4: fargate-bridge: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>1500</span> qdisc noqueue state UP mode DEFAULT group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/ether 2a:56:f8:7a:ea:8e brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>5: veth752433c6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>1500</span> qdisc noqueue master fargate-bridge state UP mode DEFAULT group default 
</span></span><span style=display:flex><span>    link/ether a2:03:66:23:29:82 brd ff:ff:ff:ff:ff:ff link-netns a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29
</span></span></code></pre></div><p>The <code>ip link show</code> command gives you the list of network interfaces in a Linux system. Notice that it only shows one ENI &ldquo;eth0&rdquo; but you don&rsquo;t see the task ENI &ldquo;eth1&rdquo;.</p><p>Let&rsquo;s confirm that eth0 is my actual primary ENI by comparing its address with the info from my AWS Console:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>bash-5.2# ip address show
</span></span><span style=display:flex><span>1: ...
</span></span><span style=display:flex><span>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>9001</span> qdisc mq state UP group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/ether 02:df:6a:de:12:21 brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>    altname enp0s5
</span></span><span style=display:flex><span>    altname ens5
</span></span><span style=display:flex><span>    inet 10.194.20.158/24 metric <span style=color:#ae81ff>1024</span> brd 10.194.20.255 scope global eth0
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>    inet6 fe80::df:6aff:fede:1221/64 scope link proto kernel_ll 
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>       ...
</span></span></code></pre></div><p>Yep, I see that the IP for eth0 is <code>10.194.20.158/24</code> and I see the MAC address matches. I also noticed that eth1 is not visible from the primary network namespace—as far as the host is concerned there is only one gateway to the internet.</p><h2 id=exploring-network-namespaces>Exploring Network Namespaces<a hidden class=anchor aria-hidden=true href=#exploring-network-namespaces>#</a></h2><p>Let&rsquo;s get a list of the network namespaces running:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>bash-5.2# ip netns show
</span></span><span style=display:flex><span>a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 <span style=color:#f92672>(</span>id: 0<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>bash-5.2# ctr -n fargate.task t ls
</span></span><span style=display:flex><span>TASK                                           PID     STATUS    
</span></span><span style=display:flex><span>a333f40b6ac74e92b1541fb0a5d76f9e-0883211837    <span style=color:#ae81ff>1551</span>    RUNNING
</span></span><span style=display:flex><span>bash-5.2# 
</span></span></code></pre></div><p>The first command <code>ip netns show</code> gives me a list of all the network namespaces. The second command is <code>ctr</code>, a command line client to talk to the containerd daemon. <code>t ls</code> lists all the tasks running in the fargate.task namespace.</p><p>Yes, we&rsquo;re running the Fargate Agent under the hood :)</p><p>Let&rsquo;s see what the namespace looks like from inside the network namespace (netns) of my awsvpc task. I&rsquo;ll use a nifty tool called <code>ip netns exec</code>, which allows me to launch commands inside a network namespace.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># ip netns exec a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 ip link show</span>
</span></span><span style=display:flex><span>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>65536</span> qdisc noqueue state UNKNOWN mode DEFAULT group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span></span><span style=display:flex><span>3: eth0@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>1500</span> qdisc noqueue state UP mode DEFAULT group default 
</span></span><span style=display:flex><span>    link/ether 0a:58:a9:fe:ac:02 brd ff:ff:ff:ff:ff:ff link-netnsid <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>4: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>9001</span> qdisc mq state UP mode DEFAULT group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/ether 02:ed:2d:97:4f:29 brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>    altname enp0s6
</span></span><span style=display:flex><span>    altname ens6
</span></span></code></pre></div><p>Well, there it is. <code>eth1</code> is here! Notice that eth0 is <strong>not</strong> here. As far as the container namespace is concerned, there&rsquo;s only one ENI in this host.</p><p>Let&rsquo;s confirm by looking at the IP address assigned to <code>eth1</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># ip netns exec a333f40b6ac74e92b1541fb0a5d76f9e-02ed2d974f29 ip address show</span>
</span></span><span style=display:flex><span>1:...
</span></span><span style=display:flex><span>3:...
</span></span><span style=display:flex><span>4: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>9001</span> qdisc mq state UP group default qlen <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    link/ether 02:ed:2d:97:4f:29 brd ff:ff:ff:ff:ff:ff
</span></span><span style=display:flex><span>    altname enp0s6
</span></span><span style=display:flex><span>    altname ens6
</span></span><span style=display:flex><span>    inet 10.194.20.168/24 scope global eth1
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span><span style=display:flex><span>    inet6 fe80::ed:2dff:fe97:4f29/64 scope link proto kernel_ll 
</span></span><span style=display:flex><span>       valid_lft forever preferred_lft forever
</span></span></code></pre></div><p>Yes, eth1 has <code>10.194.20.168</code> which matches what the AWS console tells me.</p><h2 id=the-complete-network-picture>The Complete Network Picture<a hidden class=anchor aria-hidden=true href=#the-complete-network-picture>#</a></h2><p>Okay, but what about the other stuff I see in the IP command output?</p><p>Yes, good question. This is not obvious, but there&rsquo;s one more thing: there&rsquo;s a bridge from the task namespace that connects it to the task metadata service (TMDS). The namespace is connected via a veth pair. The full namespace picture looks like this:</p><p><img loading=lazy src=/img/v2-SingleBridge-Page-4.drawio-complte.png alt="Complete Network Namespace Diagram"></p><p>Now we see that <code>eth0@if5</code> is a veth (virtual ethernet) device connected to another veth in the primary interface that will route MAC frames to the fargate-bridge which acts as a virtual switch and will route MAC traffic to the task metadata server (TMDS) running on my primary namespace.</p><h2 id=what-else-goes-into-a-netns>What else goes into a netns?<a hidden class=anchor aria-hidden=true href=#what-else-goes-into-a-netns>#</a></h2><p>Besides the network interface we need to configure the namespace with everything it needs to work as an effectual copy of a Linux network stack, with its own routes, firewall rules and devices. In awsvpc tasks this means you get:</p><ul><li>A loopback interface</li><li>DNS configuration files</li><li>Routes in the iptables</li><li>A veth interface that connects to the primary netns</li></ul><h2 id=how-to-configure-network-namespaces-using-ip-commands>How to configure network namespaces using IP commands<a hidden class=anchor aria-hidden=true href=#how-to-configure-network-namespaces-using-ip-commands>#</a></h2><p>One way to configure network namespaces is to use the <a href=https://man7.org/linux/man-pages/man8/ip-netns.8.html>IP command</a> to create and configure the network namespace.</p><p>To configure a namespace like awsvpc, you need to create the netns and configure the ENI, you need to configure DNS and create a bridge from the netns to the task metadata server.</p><p>To create a netns use <code>ip netns add</code> command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo ip netns add my_container_ns
</span></span></code></pre></div><p>This command creates a new, empty network namespace named my_container_ns. It will appear as a mount point under /var/run/netns. <a href=https://lwn.net/Articles/580893/>[1]</a></p><p>If you want to configure an ENI you&rsquo;d use <code>ip link</code> commands to create, setup, add IP address, set MTU, etc. For illustrative purposes I&rsquo;ve added some scripts created with generative AI.</p><p>This looks about right (use at your own risk, this is untested code).</p><p>To create a namespace with a bridge run something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e># This script creates a network namespace, a veth pair, and a bridge between</span>
</span></span><span style=display:flex><span><span style=color:#75715e># the host and the network namespace.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>set -e
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Configuration ---</span>
</span></span><span style=display:flex><span>NS_NAME<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;testns&#34;</span>         <span style=color:#75715e># Name of the network namespace</span>
</span></span><span style=display:flex><span>VETH_HOST<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;veth-host&#34;</span>    <span style=color:#75715e># veth interface in the root namespace</span>
</span></span><span style=display:flex><span>VETH_NS<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;veth-ns&#34;</span>        <span style=color:#75715e># veth interface inside the namespace</span>
</span></span><span style=display:flex><span>BR_NAME<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;br0&#34;</span>            <span style=color:#75715e># Name of the bridge</span>
</span></span><span style=display:flex><span>IP_HOST<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;10.200.1.1/24&#34;</span>  <span style=color:#75715e># IP for host side of the bridge</span>
</span></span><span style=display:flex><span>IP_NS<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;10.200.1.2/24&#34;</span>    <span style=color:#75715e># IP for namespace side</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Cleanup any previous setup ---</span>
</span></span><span style=display:flex><span>cleanup<span style=color:#f92672>()</span> <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>    echo <span style=color:#e6db74>&#34;Cleaning up any previous setup...&#34;</span>
</span></span><span style=display:flex><span>    ip netns del <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span> 2&gt;/dev/null <span style=color:#f92672>||</span> true
</span></span><span style=display:flex><span>    ip link del <span style=color:#e6db74>&#34;</span>$VETH_HOST<span style=color:#e6db74>&#34;</span> 2&gt;/dev/null <span style=color:#f92672>||</span> true
</span></span><span style=display:flex><span>    ip link del <span style=color:#e6db74>&#34;</span>$BR_NAME<span style=color:#e6db74>&#34;</span> 2&gt;/dev/null <span style=color:#f92672>||</span> true
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>cleanup
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Create namespace ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Creating network namespace: </span>$NS_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip netns add <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Create veth pair ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Creating veth pair: </span>$VETH_HOST<span style=color:#e6db74> &lt;-&gt; </span>$VETH_NS<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip link add <span style=color:#e6db74>&#34;</span>$VETH_HOST<span style=color:#e6db74>&#34;</span> type veth peer name <span style=color:#e6db74>&#34;</span>$VETH_NS<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Move one end into the namespace ---</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$VETH_NS<span style=color:#e6db74>&#34;</span> netns <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Create and configure bridge ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Creating bridge: </span>$BR_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip link add name <span style=color:#e6db74>&#34;</span>$BR_NAME<span style=color:#e6db74>&#34;</span> type bridge
</span></span><span style=display:flex><span>ip addr add <span style=color:#e6db74>&#34;</span>$IP_HOST<span style=color:#e6db74>&#34;</span> dev <span style=color:#e6db74>&#34;</span>$BR_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$BR_NAME<span style=color:#e6db74>&#34;</span> up
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Attach host veth to the bridge ---</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$VETH_HOST<span style=color:#e6db74>&#34;</span> master <span style=color:#e6db74>&#34;</span>$BR_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$VETH_HOST<span style=color:#e6db74>&#34;</span> up
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Configure namespace side ---</span>
</span></span><span style=display:flex><span>ip netns exec <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span> ip addr add <span style=color:#e6db74>&#34;</span>$IP_NS<span style=color:#e6db74>&#34;</span> dev <span style=color:#e6db74>&#34;</span>$VETH_NS<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip netns exec <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span> ip link set <span style=color:#e6db74>&#34;</span>$VETH_NS<span style=color:#e6db74>&#34;</span> up
</span></span><span style=display:flex><span>ip netns exec <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span> ip link set lo up
</span></span><span style=display:flex><span>ip netns exec <span style=color:#e6db74>&#34;</span>$NS_NAME<span style=color:#e6db74>&#34;</span> ip route add default via <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>IP_HOST%/*<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Enable IP forwarding (optional) ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Enabling IP forwarding...&#34;</span>
</span></span><span style=display:flex><span>sysctl -w net.ipv4.ip_forward<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> &gt;/dev/null
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;✅ Setup complete!&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Namespace: </span>$NS_NAME<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Bridge:    </span>$BR_NAME<span style=color:#e6db74> (</span>$IP_HOST<span style=color:#e6db74>)&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;veth pair: </span>$VETH_HOST<span style=color:#e6db74> &lt;-&gt; </span>$VETH_NS<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;To test connectivity:&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;  ip netns exec </span>$NS_NAME<span style=color:#e6db74> ping -c 3 </span><span style=color:#e6db74>${</span>IP_HOST%/*<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;To enter the namespace shell:&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;  ip netns exec </span>$NS_NAME<span style=color:#e6db74> bash&#34;</span>
</span></span></code></pre></div><p>To setup an ENI do something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e># Configure an AWS Elastic Network Interface (ENI)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># using &#39;ip link set&#39;, &#39;ip address add&#39;, and &#39;ip link set up&#39;.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>set -euo pipefail
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- User configuration ---</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Change these to match your setup</span>
</span></span><span style=display:flex><span>ENI_IFACE<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;eth1&#34;</span>           <span style=color:#75715e># The interface name of the ENI (check with `ip link`)</span>
</span></span><span style=display:flex><span>ENI_IP<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;10.0.2.50/24&#34;</span>      <span style=color:#75715e># Private IP address for the ENI</span>
</span></span><span style=display:flex><span>ENI_GW<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;10.0.2.1&#34;</span>          <span style=color:#75715e># Default gateway (in ENI&#39;s subnet)</span>
</span></span><span style=display:flex><span>ROUTE_TABLE<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;main&#34;</span>         <span style=color:#75715e># Optional: routing table to use</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Validate interface existence ---</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> ! ip link show <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span> &amp;&gt;/dev/null; <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>    echo <span style=color:#e6db74>&#34;❌ Error: Interface </span>$ENI_IFACE<span style=color:#e6db74> not found. Check &#39;ip link&#39; output.&#34;</span>
</span></span><span style=display:flex><span>    exit <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>fi</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;🔧 Configuring ENI interface: </span>$ENI_IFACE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Bring interface down before configuring (optional safety) ---</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span> down
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Assign IP address ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;➡️  Assigning IP address: </span>$ENI_IP<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip address flush dev <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip address add <span style=color:#e6db74>&#34;</span>$ENI_IP<span style=color:#e6db74>&#34;</span> dev <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Bring interface up ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;⬆️  Bringing interface up...&#34;</span>
</span></span><span style=display:flex><span>ip link set <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span> up
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- Add default route (optional) ---</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;🛣️  Setting default route via </span>$ENI_GW<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip route replace default via <span style=color:#e6db74>&#34;</span>$ENI_GW<span style=color:#e6db74>&#34;</span> dev <span style=color:#e6db74>&#34;</span>$ENI_IFACE<span style=color:#e6db74>&#34;</span> table <span style=color:#e6db74>&#34;</span>$ROUTE_TABLE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>ip rule add from <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>ENI_IP%/*<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> lookup <span style=color:#e6db74>&#34;</span>$ROUTE_TABLE<span style=color:#e6db74>&#34;</span> <span style=color:#f92672>||</span> true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;✅ ENI configuration complete!&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Interface: </span>$ENI_IFACE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;IP:        </span>$ENI_IP<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Gateway:   </span>$ENI_GW<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;To verify:&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;  ip addr show dev </span>$ENI_IFACE<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;  ip route show table </span>$ROUTE_TABLE<span style=color:#e6db74>&#34;</span>
</span></span></code></pre></div><h2 id=cni-plugins>CNI Plugins<a hidden class=anchor aria-hidden=true href=#cni-plugins>#</a></h2><p>Luckily, we don&rsquo;t have to use IP tools (I believe we used to before CNI plugins) since containerD relies on external plugins for network configuration. Unlike Docker users, containerD users have the freedom to customize the networking environment.</p><p>The CNI plugins and the netlib package in the ecs-agent shared library encapsulate the complexity of setting up network namespaces. The caller makes two method calls: one to build the namespace configuration and one to &ldquo;Start&rdquo; the namespace. The high level sequence of events from Start goes something like this:</p><p><img loading=lazy src=/img/cni-seq.png alt="CNI Workflow Diagram"></p><ul><li><strong>Create Namespace</strong><ul><li>Configure DNS</li></ul></li><li><strong>Configure ENI</strong><ul><li><strong>Create Bridge</strong><ul><li>Get IP from IPAM</li><li>Setup routes with iptable entries</li></ul></li><li><strong>Configure Network Interface</strong><ul><li>Move to netns</li><li>Assign IPs</li><li>Configure iptables and routes</li></ul></li></ul></li></ul><p>The plugins mainly wrap IP commands from the netlink library. This Go library simplifies adding and removing interfaces. Netlink is the successor of ioctl, a set of system calls historically (although still supported in some distros) used to configure serial devices. Since network interfaces are usually ethernet (at least in EC2 instances) this is the way to programmatically setup the system. Our customers shouldn&rsquo;t have to care/worry since this is all done for them by the ECS dataplane.</p><h2 id=so-which-cnis-do-we-use>So which CNIs do we use?<a hidden class=anchor aria-hidden=true href=#so-which-cnis-do-we-use>#</a></h2><p>For setting up awsvpc namespaces without any special networking features (such as Service Connect, AppMesh, Multi-ENI) we use the following:</p><p><strong>ENI and Branch-ENI</strong></p><p>ECS Managed Instances and Fargate use two types of ENIs: x-ENIs (Regular ENIs in the code) and branch ENIs (ENIs that leverage trunking). They are in the ENI and branch-eni packages respectively. Under the hood the plugins call netlib to setup the links.</p><p>The <a href=https://github.com/aws/amazon-ecs-cni-plugins/tree/master/plugins/eni>ECS ENI plugin</a> configures a container&rsquo;s network namespace to use an Elastic Network Interface (ENI) directly. It moves an ENI from the host into the container&rsquo;s network namespace.</p><h2 id=key-components>Key Components<a hidden class=anchor aria-hidden=true href=#key-components>#</a></h2><h3 id=engine-interface>Engine Interface<a hidden class=anchor aria-hidden=true href=#engine-interface>#</a></h3><p>The core logic is in the <code>engine</code> package, which handles:</p><ul><li>Retrieving ENI metadata from EC2 instance metadata service</li><li>Finding the network device by MAC address</li><li>Setting up the container namespace with the ENI</li></ul><h3 id=main-flow>Main Flow<a hidden class=anchor aria-hidden=true href=#main-flow>#</a></h3><ol><li><p><strong>Plugin Entry</strong>: The main function uses CNI&rsquo;s <code>skel.PluginMain()</code> to handle ADD/DEL commands</p></li><li><p><strong>ADD Command Process</strong>:</p><ul><li>Parses configuration (ENI ID, MAC address, IP addresses)</li><li>Uses EC2 metadata service to find the ENI&rsquo;s network device name</li><li>Calls <code>SetupContainerNamespace()</code> to move the ENI into the container</li></ul></li><li><p><strong>Key Engine Operations</strong>:</p><ul><li><code>GetInterfaceDeviceName()</code>: Finds the host device name using the MAC address</li><li><code>GetIPV4GatewayNetmask()</code>: Retrieves gateway/netmask from EC2 metadata</li><li><code>GetIPV6Gateway()</code>: Gets IPv6 gateway from routing table</li><li><code>SetupContainerNamespace()</code>: Moves the ENI into container&rsquo;s network namespace</li></ul></li></ol><h2 id=configuration-parameters>Configuration Parameters<a hidden class=anchor aria-hidden=true href=#configuration-parameters>#</a></h2><p>The plugin accepts:</p><ul><li><code>eni</code>: ENI ID (required)</li><li><code>mac</code>: MAC address (required)</li><li><code>ipv4-address</code>: Primary IPv4 address (required)</li><li><code>ipv6-address</code>: IPv6 address (optional)</li><li><code>block-instance-metadata</code>: Block metadata access (optional)</li><li><code>stay-down</code>: Keep interface down (optional)</li></ul><h2 id=how-it-works>How It Works<a hidden class=anchor aria-hidden=true href=#how-it-works>#</a></h2><ol><li>The plugin receives an ENI that&rsquo;s already attached to the EC2 instance</li><li>It uses the MAC address to find the corresponding network interface on the host</li><li>It retrieves network configuration (gateway, netmask) from EC2 metadata service</li><li>It moves the entire ENI device into the container&rsquo;s network namespace</li><li>Optionally starts DHCP client for lease renewal</li></ol><p>This gives containers direct access to ENIs with their own IP addresses, enabling advanced networking features like security groups per container.</p><h1 id=ecs-bridge-and-ipam-plugins>ECS Bridge and IPAM Plugins<a hidden class=anchor aria-hidden=true href=#ecs-bridge-and-ipam-plugins>#</a></h1><h2 id=ecs-bridge-plugin><a href=https://github.com/aws/amazon-ecs-cni-plugins/tree/master/plugins/ecs-bridge>ECS Bridge Plugin</a><a hidden class=anchor aria-hidden=true href=#ecs-bridge-plugin>#</a></h2><h3 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h3><p>The ECS Bridge plugin creates a bridge network to connect containers to the ECS Agent&rsquo;s credentials endpoint. It creates a bridge device and veth pairs to enable communication between containers and the host.</p><h3 id=key-components-1>Key Components<a hidden class=anchor aria-hidden=true href=#key-components-1>#</a></h3><p><strong>Main Flow</strong>:</p><ol><li><strong>Bridge Creation</strong>: Creates or reuses a bridge device (e.g., <code>ecs-br0</code>)</li><li><strong>Veth Pair Creation</strong>: Creates a virtual ethernet pair - one end in container namespace, one on host</li><li><strong>IPAM Integration</strong>: Calls the ECS IPAM plugin to allocate IP addresses</li><li><strong>Interface Configuration</strong>: Configures both ends of the veth pair with IP addresses and routes</li></ol><p><strong>Key Operations</strong>:</p><ul><li><code>CreateBridge()</code>: Creates the bridge device with specified MTU</li><li><code>CreateVethPair()</code>: Creates veth pair connecting container to host</li><li><code>AttachHostVethInterfaceToBridge()</code>: Connects host veth to bridge</li><li><code>RunIPAMPluginAdd()</code>: Calls IPAM plugin for IP allocation</li><li><code>ConfigureContainerVethInterface()</code>: Sets up container&rsquo;s network interface</li><li><code>ConfigureBridge()</code>: Configures bridge with gateway IP</li></ul><h3 id=configuration-parameters-1>Configuration Parameters<a hidden class=anchor aria-hidden=true href=#configuration-parameters-1>#</a></h3><ul><li><code>bridge</code>: Bridge name (required)</li><li><code>ipam</code>: IPAM configuration (required)</li><li><code>mtu</code>: Maximum transmission unit (optional)</li></ul><h2 id=ecs-ipam-plugin><a href=https://github.com/aws/amazon-ecs-cni-plugins/tree/master/plugins/ipam>ECS IPAM Plugin</a><a hidden class=anchor aria-hidden=true href=#ecs-ipam-plugin>#</a></h2><h3 id=overview-1>Overview<a hidden class=anchor aria-hidden=true href=#overview-1>#</a></h3><p>The ECS IPAM (IP Address Management) plugin allocates IP addresses from a specified subnet and manages IP address assignments using a BoltDB database for persistence.</p><h3 id=key-components-2>Key Components<a hidden class=anchor aria-hidden=true href=#key-components-2>#</a></h3><p><strong>IP Management</strong>:</p><ul><li>Uses BoltDB for persistent IP address tracking</li><li>Allocates IPs from a configured subnet (e.g., <code>169.254.172.0/22</code>)</li><li>Tracks last known IP to optimize allocation</li><li>Supports both automatic and manual IP assignment</li></ul><p><strong>Main Operations</strong>:</p><ul><li><code>Add()</code>: Allocates an IP address and returns network configuration</li><li><code>Del()</code>: Releases an IP address back to the pool</li><li><code>GetAvailableIP()</code>: Finds next available IP in subnet</li><li><code>Assign()</code>: Marks specific IP as used</li><li><code>Release()</code>: Frees an IP address</li></ul><h3 id=configuration-parameters-2>Configuration Parameters<a hidden class=anchor aria-hidden=true href=#configuration-parameters-2>#</a></h3><ul><li><code>ipv4-subnet</code>: CIDR block for allocations (required)</li><li><code>ipv4-address</code>: Specific IP to assign (optional)</li><li><code>ipv4-gateway</code>: Gateway IP (optional, defaults to .1)</li><li><code>ipv4-routes</code>: Routes to add to container (optional)</li><li><code>id</code>: Unique identifier for IP assignment (optional)</li></ul><h2 id=how-they-work-together>How They Work Together<a hidden class=anchor aria-hidden=true href=#how-they-work-together>#</a></h2><ol><li><strong>Bridge Plugin Invocation</strong>: ECS Agent calls bridge plugin with configuration</li><li><strong>Bridge Setup</strong>: Plugin creates bridge device and veth pair</li><li><strong>IPAM Call</strong>: Bridge plugin calls IPAM plugin to get IP allocation</li><li><strong>IP Assignment</strong>: IPAM plugin allocates IP from subnet and stores in database</li><li><strong>Interface Configuration</strong>: Bridge plugin configures container interface with allocated IP</li><li><strong>Route Setup</strong>: Adds routes for ECS credentials endpoint communication</li></ol><p>This enables containers to communicate with the ECS Agent&rsquo;s credentials endpoint at <code>169.254.170.2</code> while maintaining network isolation.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>Diving into ECS networking internals shows how sophisticated systems can appear simple from the outside. Network namespaces provide the isolation, CNI plugins manage the complexity, and developers get clean abstractions. Sometimes the
best engineering is the kind you never have to think about.</p><h1 id=further-resources>Further resources<a hidden class=anchor aria-hidden=true href=#further-resources>#</a></h1><h2 id=ecs-documentation>ECS Documentation<a hidden class=anchor aria-hidden=true href=#ecs-documentation>#</a></h2><p><a href=https://docs.aws.amazon.com/AmazonECS/latest/developerguide/managed-instance-networking.html>Networking for ECS Managed Instances</a></p><h2 id=ecs-agent-netlib-library>ECS Agent netlib library<a hidden class=anchor aria-hidden=true href=#ecs-agent-netlib-library>#</a></h2><p>This is where the code that invokes the CNIs live:
<a href=https://github.com/aws/amazon-ecs-agent/tree/master/ecs-agent/netlib>netlib</a></p><h2 id=cni-plugins-source-code>CNI plugins source code<a hidden class=anchor aria-hidden=true href=#cni-plugins-source-code>#</a></h2><p>This package contains the bridge, eni and IPAM plugins, which are the &ldquo;bread and butter&rdquo; of network configuration. They are used by both Fargate and Managed Instances:
<a href=https://github.com/aws/amazon-ecs-cni-plugins/tree/master/plugins>ecs-cni-plugins</a></p><p>Special CNIs that are aws vpc specific. Managed Instances make use of the vpc-branch-eni plugin to configure trunk and branch ENIs. <a href=https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-eni.html>Trunking</a> is an AWS feature that allows for more task density on hosts by giving the capability to pack more ENIs per instance.</p><p><a href=https://github.com/aws/amazon-vpc-cni-plugins/tree/master/plugins/vpc-branch-eni>vpc-cni-plugins</a></p><h2 id=ip-command>IP Command<a hidden class=anchor aria-hidden=true href=#ip-command>#</a></h2><p>Source Code for iproute, the networking utility that powers the CNI plugins:
<a href="https://github.com/iproute2/iproute2?tab=readme-ov-file">iproute2</a></p><p>Netlink Protocol
<a href=https://docs.kernel.org/userspace-api/netlink/intro.html>netlink</a></p><h2 id=linux-networking-resources>Linux Networking Resources<a hidden class=anchor aria-hidden=true href=#linux-networking-resources>#</a></h2><p>Official documentation
<a href=https://linux-kernel-labs.github.io/refs/heads/master/labs/networking.html>networking</a></p><p>Free Online Books
<a href=https://book.systemsapproach.org/>Computer Networks: A System Approach</a>
<a href=https://sdn.systemsapproach.org/>Software-Defined Networks: A Systems Approach</a></p><h2 id=my-favorite-networking-books>My favorite networking books<a hidden class=anchor aria-hidden=true href=#my-favorite-networking-books>#</a></h2><p><a href=https://www.amazon.com/Understanding-Linux-Network-Internals-Networking/dp/0596002556>Understanding Linux Network Internals: Guided Tour to Networking on Linux</a> is a bit dated but still good source of background.</p><p>I saw this book in <a href=https://jvns.ca/bookshelf/>Julia Evans bookshelf</a> Julia is an excellent software engineer and gifted technical writer, I cannot recommend reading her blog highly enough.
<a href=https://www.amazon.com/Networking-Systems-Administrators-Mastery-Book-ebook/dp/B00STLTH74>Networking for System Administrators</a></p><p>Alright, this post is getting long, I&rsquo;ll stop now, but come back for the next installation where I&rsquo;ll go over what I diddn&rsquo;t get a chance to get into.</p><hr><p><em>This deep dive into ECS networking shows the sophisticated engineering that makes container isolation possible. Network namespaces provide the foundation for secure, isolated networking while CNI plugins abstract away the complexity for developers. Understanding these internals helps appreciate the elegant solutions that power modern container orchestration.</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://JoseVillalta.github.io/tags/aws/>Aws</a></li><li><a href=https://JoseVillalta.github.io/tags/ecs/>Ecs</a></li><li><a href=https://JoseVillalta.github.io/tags/containers/>Containers</a></li><li><a href=https://JoseVillalta.github.io/tags/networking/>Networking</a></li><li><a href=https://JoseVillalta.github.io/tags/linux/>Linux</a></li><li><a href=https://JoseVillalta.github.io/tags/namespaces/>Namespaces</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://JoseVillalta.github.io/>Yes Way Jose</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>